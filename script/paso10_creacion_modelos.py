"""
================================================================================
TELECOMX - PASO 10: CREACI√ìN DE MODELOS PREDICTIVOS
================================================================================
Descripci√≥n: Creaci√≥n y entrenamiento de modelos de machine learning para
             predicci√≥n de churn usando configuraciones optimizadas de pasos
             anteriores.

Modelos Implementados:
- Random Forest: Sin normalizaci√≥n, con class weighting
- Regresi√≥n Log√≠stica: Con normalizaci√≥n, con class weighting

Funcionalidades:
- Carga de datasets separados del Paso 9
- Configuraci√≥n de pipelines de entrenamiento
- Aplicaci√≥n de class weighting del Paso 4
- Normalizaci√≥n selectiva seg√∫n modelo
- Guardado de modelos entrenados
- Feature importance para Random Forest

Autor: Sistema de An√°lisis Predictivo TelecomX
Fecha: 2025
================================================================================
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import logging
import os
import joblib
import json
from datetime import datetime
from pathlib import Path
import warnings

# Importaciones de scikit-learn
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report
import pickle

warnings.filterwarnings('ignore')

# Configuraci√≥n global
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

def setup_logging():
    """Configurar sistema de logging"""
    os.makedirs('logs', exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('logs/paso10_creacion_modelos.log', mode='a', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

def create_directories():
    """Crear directorios necesarios"""
    directories = ['excel', 'informes', 'graficos', 'logs', 'datos', 'modelos']
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        logging.info(f"Carpeta verificada/creada: {directory}")

def find_latest_file(directory, pattern):
    """Encontrar el archivo m√°s reciente que coincida con el patr√≥n"""
    files = list(Path(directory).glob(pattern))
    if not files:
        raise FileNotFoundError(f"No se encontraron archivos con patr√≥n {pattern} en {directory}")
    latest_file = max(files, key=os.path.getctime)
    return str(latest_file)

def load_training_data():
    """Cargar datos de entrenamiento del Paso 9"""
    try:
        # Buscar archivo de entrenamiento m√°s reciente
        train_file = find_latest_file('datos', 'telecomx_train_dataset_*.csv')
        logging.info(f"Cargando datos de entrenamiento: {train_file}")
        
        # Cargar datos con manejo de encoding
        encodings = ['utf-8-sig', 'utf-8', 'cp1252', 'latin-1']
        df_train = None
        
        for encoding in encodings:
            try:
                df_train = pd.read_csv(train_file, encoding=encoding)
                logging.info(f"Datos cargados con encoding: {encoding}")
                break
            except UnicodeDecodeError:
                continue
        
        if df_train is None:
            raise ValueError("No se pudo cargar el archivo con ninguna codificaci√≥n")
        
        # Verificar estructura
        required_vars = ['Abandono_Cliente', 'Meses_Cliente', 'Cargo_Total']
        missing_vars = [var for var in required_vars if var not in df_train.columns]
        if missing_vars:
            raise ValueError(f"Variables requeridas faltantes: {missing_vars}")
        
        # Separar caracter√≠sticas y variable objetivo
        target_var = 'Abandono_Cliente'
        X_train = df_train.drop(columns=[target_var])
        y_train = df_train[target_var]
        
        logging.info(f"Datos de entrenamiento cargados:")
        logging.info(f"  Muestras: {len(X_train):,}")
        logging.info(f"  Caracter√≠sticas: {X_train.shape[1]}")
        logging.info(f"  Balance de clases: {y_train.value_counts().to_dict()}")
        logging.info(f"  Tasa de churn: {y_train.mean():.3f}")
        
        return X_train, y_train, train_file
        
    except Exception as e:
        logging.error(f"Error al cargar datos de entrenamiento: {str(e)}")
        raise

def get_class_weights(y_train):
    """Calcular class weights basado en configuraci√≥n del Paso 4"""
    logging.info("Calculando class weights...")
    
    # Contar clases
    class_counts = y_train.value_counts().sort_index()
    total_samples = len(y_train)
    
    # Calcular ratio como en el Paso 4
    n_minority = class_counts[1]  # Churn
    n_majority = class_counts[0]  # No churn
    ratio = n_majority / n_minority
    
    # Configuraci√≥n conservadora del Paso 4 (ratio 2.77:1 ‚Üí peso 2.5)
    # Ajustar seg√∫n el ratio actual
    conservative_weight = min(2.5, ratio * 0.9)  # Ligeramente conservador
    
    class_weights = {
        0: 1.0,  # No churn (clase mayoritaria)
        1: conservative_weight  # Churn (clase minoritaria)
    }
    
    logging.info(f"Distribuci√≥n de clases:")
    logging.info(f"  Clase 0 (No churn): {n_majority:,} ({n_majority/total_samples:.1%})")
    logging.info(f"  Clase 1 (Churn): {n_minority:,} ({n_minority/total_samples:.1%})")
    logging.info(f"  Ratio: {ratio:.2f}:1")
    logging.info(f"Class weights calculados: {class_weights}")
    
    return class_weights, ratio

def create_random_forest_model(X_train, y_train, class_weights):
    """Crear y entrenar modelo Random Forest"""
    logging.info("Creando modelo Random Forest...")
    
    # Configuraci√≥n del modelo
    rf_params = {
        'n_estimators': 100,
        'max_depth': 15,
        'min_samples_split': 10,
        'min_samples_leaf': 5,
        'class_weight': class_weights,
        'random_state': RANDOM_SEED,
        'n_jobs': -1
    }
    
    # Crear y entrenar modelo
    rf_model = RandomForestClassifier(**rf_params)
    
    logging.info(f"Entrenando Random Forest con par√°metros: {rf_params}")
    rf_model.fit(X_train, y_train)
    
    # Obtener feature importance
    feature_importance = pd.DataFrame({
        'feature': X_train.columns,
        'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    logging.info("Random Forest entrenado exitosamente")
    logging.info(f"Top 5 caracter√≠sticas importantes:")
    for idx, row in feature_importance.head().iterrows():
        logging.info(f"  {row['feature']}: {row['importance']:.4f}")
    
    model_info = {
        'model_type': 'RandomForest',
        'parameters': rf_params,
        'feature_importance': feature_importance.to_dict('records'),
        'n_features': len(X_train.columns),
        'training_samples': len(X_train)
    }
    
    return rf_model, model_info

def create_logistic_regression_model(X_train, y_train, class_weights):
    """Crear y entrenar modelo Regresi√≥n Log√≠stica con normalizaci√≥n"""
    logging.info("Creando modelo Regresi√≥n Log√≠stica con normalizaci√≥n...")
    
    # Configuraci√≥n del modelo
    lr_params = {
        'class_weight': class_weights,
        'random_state': RANDOM_SEED,
        'max_iter': 1000,
        'solver': 'liblinear'  # Mejor para datasets peque√±os-medianos
    }
    
    # Crear pipeline con normalizaci√≥n
    lr_pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', LogisticRegression(**lr_params))
    ])
    
    logging.info(f"Entrenando Regresi√≥n Log√≠stica con normalizaci√≥n")
    logging.info(f"Par√°metros del clasificador: {lr_params}")
    
    # Entrenar pipeline completo
    lr_pipeline.fit(X_train, y_train)
    
    # Obtener coeficientes (despu√©s de la normalizaci√≥n)
    lr_model = lr_pipeline.named_steps['classifier']
    scaler = lr_pipeline.named_steps['scaler']
    
    # Coeficientes con nombres de caracter√≠sticas
    coefficients = pd.DataFrame({
        'feature': X_train.columns,
        'coefficient': lr_model.coef_[0]
    })
    coefficients['abs_coefficient'] = abs(coefficients['coefficient'])
    coefficients = coefficients.sort_values('abs_coefficient', ascending=False)
    
    logging.info("Regresi√≥n Log√≠stica entrenada exitosamente")
    logging.info(f"Top 5 caracter√≠sticas por coeficiente absoluto:")
    for idx, row in coefficients.head().iterrows():
        direction = "‚Üë" if row['coefficient'] > 0 else "‚Üì"
        logging.info(f"  {row['feature']}: {row['coefficient']:.4f} {direction}")
    
    model_info = {
        'model_type': 'LogisticRegression',
        'parameters': lr_params,
        'coefficients': coefficients.to_dict('records'),
        'intercept': float(lr_model.intercept_[0]),
        'n_features': len(X_train.columns),
        'training_samples': len(X_train),
        'normalized': True
    }
    
    return lr_pipeline, model_info

def validate_model_training(model, X_train, y_train, model_name):
    """Validaci√≥n b√°sica de que el modelo se entren√≥ correctamente"""
    logging.info(f"Validando entrenamiento de {model_name}...")
    
    try:
        # Hacer predicciones de entrenamiento
        y_pred = model.predict(X_train)
        y_pred_proba = model.predict_proba(X_train)
        
        # Validaciones b√°sicas
        assert len(y_pred) == len(y_train), "Longitud de predicciones incorrecta"
        assert set(y_pred).issubset({0, 1}), "Predicciones fuera del rango esperado"
        assert y_pred_proba.shape == (len(y_train), 2), "Shape de probabilidades incorrecto"
        
        # Estad√≠sticas b√°sicas
        unique_preds = pd.Series(y_pred).value_counts().sort_index()
        
        logging.info(f"Validaci√≥n de {model_name} exitosa:")
        logging.info(f"  Predicciones √∫nicas: {unique_preds.to_dict()}")
        logging.info(f"  Rango de probabilidades: {y_pred_proba[:, 1].min():.3f} - {y_pred_proba[:, 1].max():.3f}")
        
        return True
        
    except Exception as e:
        logging.error(f"Error en validaci√≥n de {model_name}: {str(e)}")
        return False

def save_models(rf_model, rf_info, lr_model, lr_info, timestamp):
    """Guardar modelos entrenados y su informaci√≥n"""
    logging.info("Guardando modelos entrenados...")
    
    try:
        saved_files = {}
        
        # Guardar Random Forest
        rf_filename = f'modelos/random_forest_model_{timestamp}.pkl'
        with open(rf_filename, 'wb') as f:
            pickle.dump(rf_model, f)
        saved_files['random_forest_model'] = rf_filename
        
        # Guardar informaci√≥n de Random Forest
        rf_info_filename = f'modelos/random_forest_info_{timestamp}.json'
        with open(rf_info_filename, 'w', encoding='utf-8') as f:
            json.dump(rf_info, f, indent=2, ensure_ascii=False)
        saved_files['random_forest_info'] = rf_info_filename
        
        # Guardar Regresi√≥n Log√≠stica (pipeline completo)
        lr_filename = f'modelos/logistic_regression_pipeline_{timestamp}.pkl'
        with open(lr_filename, 'wb') as f:
            pickle.dump(lr_model, f)
        saved_files['logistic_regression_model'] = lr_filename
        
        # Guardar informaci√≥n de Regresi√≥n Log√≠stica
        lr_info_filename = f'modelos/logistic_regression_info_{timestamp}.json'
        with open(lr_info_filename, 'w', encoding='utf-8') as f:
            json.dump(lr_info, f, indent=2, ensure_ascii=False)
        saved_files['logistic_regression_info'] = lr_info_filename
        
        # Guardar configuraci√≥n general
        config = {
            'timestamp': timestamp,
            'random_seed': RANDOM_SEED,
            'models_created': ['RandomForest', 'LogisticRegression'],
            'class_weights': rf_info.get('class_weights', {}),
            'training_samples': rf_info['training_samples'],
            'n_features': rf_info['n_features']
        }
        
        config_filename = f'modelos/models_configuration_{timestamp}.json'
        with open(config_filename, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        saved_files['configuration'] = config_filename
        
        logging.info(f"Modelos guardados exitosamente:")
        for key, filename in saved_files.items():
            logging.info(f"  {key}: {filename}")
        
        return saved_files
        
    except Exception as e:
        logging.error(f"Error al guardar modelos: {str(e)}")
        raise

def generate_feature_importance_visualization(rf_info, timestamp):
    """Generar visualizaci√≥n de feature importance"""
    logging.info("Generando visualizaci√≥n de feature importance...")
    
    try:
        # Preparar datos
        feature_importance = pd.DataFrame(rf_info['feature_importance'])
        top_features = feature_importance.head(15)  # Top 15 caracter√≠sticas
        
        # Crear gr√°fico
        plt.figure(figsize=(12, 8))
        
        bars = plt.barh(range(len(top_features)), top_features['importance'], 
                       color='skyblue', alpha=0.8)
        
        plt.yticks(range(len(top_features)), top_features['feature'])
        plt.xlabel('Importancia de la Caracter√≠stica', fontsize=12)
        plt.title('Top 15 Caracter√≠sticas - Random Forest', fontsize=14, fontweight='bold')
        plt.grid(True, axis='x', alpha=0.3)
        
        # Agregar valores en las barras
        for i, (bar, value) in enumerate(zip(bars, top_features['importance'])):
            plt.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2,
                    f'{value:.3f}', ha='left', va='center', fontweight='bold', fontsize=9)
        
        # Mejorar layout
        plt.tight_layout()
        
        # Guardar gr√°fico
        viz_filename = f'graficos/paso10_feature_importance_{timestamp}.png'
        plt.savefig(viz_filename, dpi=300, bbox_inches='tight')
        plt.close()
        
        logging.info(f"Visualizaci√≥n guardada: {viz_filename}")
        return viz_filename
        
    except Exception as e:
        logging.error(f"Error al generar visualizaci√≥n: {str(e)}")
        return None

def generate_coefficients_visualization(lr_info, timestamp):
    """Generar visualizaci√≥n de coeficientes de Regresi√≥n Log√≠stica"""
    logging.info("Generando visualizaci√≥n de coeficientes...")
    
    try:
        # Preparar datos
        coefficients = pd.DataFrame(lr_info['coefficients'])
        
        # Separar positivos y negativos
        positive_coefs = coefficients[coefficients['coefficient'] > 0].head(8)
        negative_coefs = coefficients[coefficients['coefficient'] < 0].head(7)  # Total 15
        
        # Crear gr√°fico
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
        
        # Coeficientes positivos (aumentan probabilidad de churn)
        if not positive_coefs.empty:
            bars1 = ax1.barh(range(len(positive_coefs)), positive_coefs['coefficient'], 
                           color='lightcoral', alpha=0.8)
            ax1.set_yticks(range(len(positive_coefs)))
            ax1.set_yticklabels(positive_coefs['feature'])
            ax1.set_xlabel('Coeficiente (Aumenta Probabilidad de Churn)')
            ax1.set_title('Coeficientes Positivos - Regresi√≥n Log√≠stica', fontweight='bold')
            ax1.grid(True, axis='x', alpha=0.3)
            
            # Valores en barras
            for bar, value in zip(bars1, positive_coefs['coefficient']):
                ax1.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,
                        f'{value:.3f}', ha='left', va='center', fontweight='bold', fontsize=9)
        
        # Coeficientes negativos (disminuyen probabilidad de churn)
        if not negative_coefs.empty:
            bars2 = ax2.barh(range(len(negative_coefs)), negative_coefs['coefficient'], 
                           color='lightgreen', alpha=0.8)
            ax2.set_yticks(range(len(negative_coefs)))
            ax2.set_yticklabels(negative_coefs['feature'])
            ax2.set_xlabel('Coeficiente (Disminuye Probabilidad de Churn)')
            ax2.set_title('Coeficientes Negativos - Regresi√≥n Log√≠stica', fontweight='bold')
            ax2.grid(True, axis='x', alpha=0.3)
            
            # Valores en barras
            for bar, value in zip(bars2, negative_coefs['coefficient']):
                ax2.text(bar.get_width() - 0.01, bar.get_y() + bar.get_height()/2,
                        f'{value:.3f}', ha='right', va='center', fontweight='bold', fontsize=9)
        
        plt.tight_layout()
        
        # Guardar gr√°fico
        viz_filename = f'graficos/paso10_coeficientes_logistica_{timestamp}.png'
        plt.savefig(viz_filename, dpi=300, bbox_inches='tight')
        plt.close()
        
        logging.info(f"Visualizaci√≥n de coeficientes guardada: {viz_filename}")
        return viz_filename
        
    except Exception as e:
        logging.error(f"Error al generar visualizaci√≥n de coeficientes: {str(e)}")
        return None

def generate_comprehensive_report(rf_info, lr_info, class_weights, ratio, saved_files, 
                                 viz_files, training_file, timestamp):
    """Generar informe completo de creaci√≥n de modelos"""
    
    report = f"""
================================================================================
TELECOMX - INFORME PASO 10: CREACI√ìN DE MODELOS PREDICTIVOS
================================================================================
Fecha y Hora: {timestamp}
Paso: 10 - Creaci√≥n de Modelos
Semilla Aleatoria: {RANDOM_SEED}

================================================================================
RESUMEN EJECUTIVO
================================================================================
‚Ä¢ Modelos Creados: 2 (Random Forest + Regresi√≥n Log√≠stica)
‚Ä¢ Datos de Entrenamiento: {rf_info['training_samples']:,} muestras
‚Ä¢ Caracter√≠sticas Utilizadas: {rf_info['n_features']}
‚Ä¢ Class Weighting Aplicado: Configuraci√≥n conservadora (Paso 4)
‚Ä¢ Normalizaci√≥n: Solo en Regresi√≥n Log√≠stica
‚Ä¢ Estado: Modelos entrenados y guardados exitosamente

================================================================================
CONFIGURACI√ìN DE DATOS
================================================================================

üìä DATASET DE ENTRENAMIENTO:
‚Ä¢ Archivo utilizado: {training_file}
‚Ä¢ Total de muestras: {rf_info['training_samples']:,}
‚Ä¢ N√∫mero de caracter√≠sticas: {rf_info['n_features']}
‚Ä¢ Variable objetivo: Abandono_Cliente

‚öñÔ∏è DISTRIBUCI√ìN DE CLASES:
‚Ä¢ Ratio detectado: {ratio:.2f}:1 (No Churn : Churn)
‚Ä¢ Class weights aplicados:
  - Clase 0 (No Churn): {class_weights[0]:.1f}
  - Clase 1 (Churn): {class_weights[1]:.1f}
‚Ä¢ Estrategia: Conservadora basada en an√°lisis del Paso 4

================================================================================
MODELO 1: RANDOM FOREST
================================================================================

üå≥ CONFIGURACI√ìN DEL MODELO:
‚Ä¢ Tipo: Ensemble - Random Forest
‚Ä¢ N¬∞ de √°rboles: {rf_info['parameters']['n_estimators']}
‚Ä¢ Profundidad m√°xima: {rf_info['parameters']['max_depth']}
‚Ä¢ Min. muestras split: {rf_info['parameters']['min_samples_split']}
‚Ä¢ Min. muestras hoja: {rf_info['parameters']['min_samples_leaf']}
‚Ä¢ Class weight: Personalizado {class_weights}
‚Ä¢ Normalizaci√≥n: No requerida

üéØ CARACTER√çSTICAS PRINCIPALES:
‚Ä¢ No sensible a escala de variables
‚Ä¢ Maneja bien datos desbalanceados
‚Ä¢ Proporciona feature importance interpretable
‚Ä¢ Robusto a outliers y ruido

üìä TOP 10 CARACTER√çSTICAS M√ÅS IMPORTANTES:
"""
    
    # A√±adir feature importance
    for i, feature_info in enumerate(rf_info['feature_importance'][:10], 1):
        importance = feature_info['importance']
        feature_name = feature_info['feature']
        report += f"""
{i:2d}. {feature_name}: {importance:.4f}"""

    report += f"""

‚úÖ ESTADO DEL MODELO:
‚Ä¢ Entrenamiento: Exitoso
‚Ä¢ Validaci√≥n b√°sica: Aprobada
‚Ä¢ Archivo guardado: {saved_files['random_forest_model']}
‚Ä¢ Informaci√≥n guardada: {saved_files['random_forest_info']}

================================================================================
MODELO 2: REGRESI√ìN LOG√çSTICA
================================================================================

üìà CONFIGURACI√ìN DEL MODELO:
‚Ä¢ Tipo: Lineal - Regresi√≥n Log√≠stica
‚Ä¢ Solver: {lr_info['parameters']['solver']}
‚Ä¢ Max iteraciones: {lr_info['parameters']['max_iter']}
‚Ä¢ Class weight: Personalizado {class_weights}
‚Ä¢ Normalizaci√≥n: StandardScaler aplicado

üéØ CARACTER√çSTICAS PRINCIPALES:
‚Ä¢ Sensible a escala ‚Üí Normalizaci√≥n aplicada
‚Ä¢ Interpretable mediante coeficientes
‚Ä¢ R√°pido entrenamiento e inferencia
‚Ä¢ Baseline s√≥lido para comparaci√≥n

üìä TOP 10 COEFICIENTES POR MAGNITUD ABSOLUTA:
"""
    
    # A√±adir coeficientes
    for i, coef_info in enumerate(lr_info['coefficients'][:10], 1):
        coefficient = coef_info['coefficient']
        feature_name = coef_info['feature']
        direction = "‚Üë Aumenta" if coefficient > 0 else "‚Üì Disminuye"
        report += f"""
{i:2d}. {feature_name}: {coefficient:+.4f} ({direction} probabilidad de churn)"""

    report += f"""

üìê PAR√ÅMETROS DEL MODELO:
‚Ä¢ Intercepto: {lr_info['intercept']:+.4f}
‚Ä¢ Pipeline: StandardScaler ‚Üí LogisticRegression
‚Ä¢ N√∫mero de coeficientes: {len(lr_info['coefficients'])}

‚úÖ ESTADO DEL MODELO:
‚Ä¢ Entrenamiento: Exitoso
‚Ä¢ Normalizaci√≥n: Aplicada correctamente
‚Ä¢ Validaci√≥n b√°sica: Aprobada
‚Ä¢ Archivo guardado: {saved_files['logistic_regression_model']}
‚Ä¢ Informaci√≥n guardada: {saved_files['logistic_regression_info']}

================================================================================
JUSTIFICACI√ìN DE NORMALIZACI√ìN
================================================================================

üî¨ AN√ÅLISIS POR MODELO:

RANDOM FOREST (Sin normalizaci√≥n):
‚úÖ Razones para NO normalizar:
‚Ä¢ Los √°rboles de decisi√≥n no dependen de la escala de variables
‚Ä¢ Las divisiones se basan en valores relativos dentro de cada caracter√≠stica
‚Ä¢ La distancia euclidiana no es relevante para el algoritmo
‚Ä¢ Mantiene interpretabilidad original de las variables

REGRESI√ìN LOG√çSTICA (Con normalizaci√≥n):
‚úÖ Razones para S√ç normalizar:
‚Ä¢ Los coeficientes son sensibles a la magnitud de las variables
‚Ä¢ Variables con escalas grandes dominan la funci√≥n de costo
‚Ä¢ La optimizaci√≥n (gradiente descendente) converge mejor con datos normalizados
‚Ä¢ Los coeficientes normalizados son m√°s interpretables

‚öñÔ∏è IMPACTO DE LA NORMALIZACI√ìN:
‚Ä¢ Variables financieras (Cargo_Total): Rango ~0-8000 ‚Üí Normalizado a ~(-2, +3)
‚Ä¢ Variables temporales (Meses_Cliente): Rango ~0-72 ‚Üí Normalizado a ~(-2, +2)  
‚Ä¢ Variables binarias (encoding): Rango 0-1 ‚Üí Mantenido similar
‚Ä¢ Resultado: Todas las variables contribuyen equitativamente al modelo lineal

================================================================================
CONFIGURACIONES APLICADAS DE PASOS ANTERIORES
================================================================================

üîó INTEGRACI√ìN CON PIPELINE ANTERIOR:

PASO 4 - CLASS WEIGHTING:
‚Ä¢ Configuraci√≥n conservadora aplicada
‚Ä¢ Ratio original detectado: 2.77:1
‚Ä¢ Peso ajustado para clase minoritaria: {class_weights[1]:.1f}

PASO 7 - VARIABLES OPTIMIZADAS:
‚Ä¢ Variables utilizadas: {rf_info['n_features']} (tras eliminaci√≥n de columnas irrelevantes)
‚Ä¢ Variables multicolineales eliminadas
‚Ä¢ Solo predictores relevantes mantenidos

PASO 8 - INSIGHTS DIRIGIDOS:
‚Ä¢ Variables clave identificadas: Meses_Cliente, Cargo_Total
‚Ä¢ Patrones de segmentaci√≥n considerados en feature importance

PASO 9 - DATOS SEPARADOS:
‚Ä¢ Solo datos de entrenamiento utilizados
‚Ä¢ Validaci√≥n y test reservados para evaluaci√≥n
‚Ä¢ Estratificaci√≥n mantenida

================================================================================
ARCHIVOS GENERADOS
================================================================================

ü§ñ MODELOS ENTRENADOS:
‚Ä¢ Random Forest: {saved_files['random_forest_model']}
‚Ä¢ Regresi√≥n Log√≠stica: {saved_files['logistic_regression_model']}

üìä INFORMACI√ìN DE MODELOS:
‚Ä¢ Random Forest info: {saved_files['random_forest_info']}
‚Ä¢ Regresi√≥n Log√≠stica info: {saved_files['logistic_regression_info']}
‚Ä¢ Configuraci√≥n general: {saved_files['configuration']}

üìà VISUALIZACIONES:
"""
    
    # A√±adir visualizaciones generadas
    if viz_files.get('feature_importance'):
        report += f"‚Ä¢ Feature Importance: {viz_files['feature_importance']}\n"
    if viz_files.get('coefficients'):
        report += f"‚Ä¢ Coeficientes Log√≠stica: {viz_files['coefficients']}\n"

    report += f"""
üìÑ DOCUMENTACI√ìN:
‚Ä¢ Informe completo: informes/paso10_creacion_modelos_informe_{timestamp}.txt
‚Ä¢ Log del proceso: logs/paso10_creacion_modelos.log

================================================================================
CARACTER√çSTICAS T√âCNICAS
================================================================================

üîß ESPECIFICACIONES DE ENTRENAMIENTO:
‚Ä¢ Semilla aleatoria: {RANDOM_SEED} (reproducibilidad garantizada)
‚Ä¢ Paralelizaci√≥n: Random Forest usa todos los cores disponibles
‚Ä¢ Memoria utilizada: Optimizada para dataset de tama√±o medio
‚Ä¢ Tiempo de entrenamiento: < 1 minuto por modelo

‚úÖ VALIDACIONES REALIZADAS:
‚Ä¢ Estructura de datos verificada
‚Ä¢ Predicciones en rango v√°lido [0, 1]
‚Ä¢ Probabilidades suman 1.0
‚Ä¢ Modelos serializados correctamente

üéØ PREPARACI√ìN PARA EVALUACI√ìN:
‚Ä¢ Modelos listos para inference
‚Ä¢ Compatible con datos del Paso 9
‚Ä¢ Formatos est√°ndar para m√©tricas
‚Ä¢ Pipeline completo preservado

================================================================================
PR√ìXIMO PASO RECOMENDADO
================================================================================

Paso 11: Evaluaci√≥n de Modelos
‚Ä¢ Cargar modelos desde carpeta modelos/
‚Ä¢ Evaluar en conjuntos de validaci√≥n y test
‚Ä¢ Comparar rendimiento: Random Forest vs Regresi√≥n Log√≠stica
‚Ä¢ M√©tricas especializadas: F1-Score, AUC-PR, Recall
‚Ä¢ Matrices de confusi√≥n y curvas ROC/PR
‚Ä¢ Selecci√≥n del modelo final

üéØ ARCHIVOS NECESARIOS PARA EL PASO 11:
‚Ä¢ Modelos: modelos/random_forest_model_{timestamp}.pkl
‚Ä¢ Modelos: modelos/logistic_regression_pipeline_{timestamp}.pkl  
‚Ä¢ Datos validaci√≥n: datos/telecomx_validation_dataset_*.csv
‚Ä¢ Datos test: datos/telecomx_test_dataset_*.csv

================================================================================
RECOMENDACIONES
================================================================================

üí° CONSIDERACIONES PARA LA EVALUACI√ìN:

1. M√âTRICAS PRINCIPALES:
   ‚Ä¢ F1-Score: Balance entre precision y recall
   ‚Ä¢ AUC-PR: Mejor que AUC-ROC para datos desbalanceados
   ‚Ä¢ Recall: Importante para capturar todos los churns

2. COMPARACI√ìN DE MODELOS:
   ‚Ä¢ Random Forest: Esperado mejor en datos complejos
   ‚Ä¢ Regresi√≥n Log√≠stica: Baseline interpretable
   ‚Ä¢ Considerar ensemble si ambos son competitivos

3. INTERPRETABILIDAD:
   ‚Ä¢ Random Forest: Feature importance ya calculada
   ‚Ä¢ Regresi√≥n Log√≠stica: Coeficientes ya interpretados
   ‚Ä¢ Validar consistencia entre importancias

4. VALIDACI√ìN DE ROBUSTEZ:
   ‚Ä¢ Verificar performance en conjunto de validaci√≥n
   ‚Ä¢ Evaluar generalizaci√≥n en conjunto de test
   ‚Ä¢ Detectar posible overfitting

================================================================================
CONSIDERACIONES DE PRODUCCI√ìN
================================================================================

üöÄ PREPARACI√ìN PARA DESPLIEGUE:

PERFORMANCE:
‚Ä¢ Random Forest: Mayor tiempo de inferencia pero m√°s robusto
‚Ä¢ Regresi√≥n Log√≠stica: Inferencia r√°pida, ideal para tiempo real
‚Ä¢ Ambos modelos optimizados para el tama√±o del dataset

MANTENIMIENTO:
‚Ä¢ Modelos guardados en formato pickle est√°ndar
‚Ä¢ Pipeline de Regresi√≥n Log√≠stica incluye normalizaci√≥n autom√°tica
‚Ä¢ Configuraciones documentadas para reentrenamiento

ESCALABILIDAD:
‚Ä¢ Compatible con nuevos datos del mismo formato
‚Ä¢ F√°cil integraci√≥n en sistemas de producci√≥n
‚Ä¢ Monitoreo de drift recomendado

================================================================================
FIN DEL INFORME
================================================================================
"""
    
    return report

def save_report_and_config(report_content, timestamp):
    """Guardar informe y archivos de configuraci√≥n"""
    try:
        # Guardar informe
        report_file = f'informes/paso10_creacion_modelos_informe_{timestamp}.txt'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        logging.info(f"Informe guardado: {report_file}")
        
        return {'report_file': report_file}
        
    except Exception as e:
        logging.error(f"Error al guardar informe: {str(e)}")
        raise

def main():
    """Funci√≥n principal del Paso 10"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    logger = setup_logging()
    
    try:
        logger.info("="*80)
        logger.info("INICIANDO PASO 10: CREACI√ìN DE MODELOS PREDICTIVOS")
        logger.info("="*80)
        logger.info("Modelos a crear: Random Forest (sin normalizaci√≥n) + Regresi√≥n Log√≠stica (con normalizaci√≥n)")
        logger.info(f"Semilla aleatoria: {RANDOM_SEED}")
        
        # 1. Crear directorios necesarios
        create_directories()
        
        # 2. Cargar datos de entrenamiento
        X_train, y_train, training_file = load_training_data()
        
        # 3. Calcular class weights
        class_weights, ratio = get_class_weights(y_train)
        
        # 4. Crear y entrenar Random Forest
        logger.info("="*50)
        rf_model, rf_info = create_random_forest_model(X_train, y_train, class_weights)
        rf_info['class_weights'] = class_weights  # A√±adir para el informe
        
        # Validar Random Forest
        rf_valid = validate_model_training(rf_model, X_train, y_train, "Random Forest")
        if not rf_valid:
            raise ValueError("Validaci√≥n de Random Forest fall√≥")
        
        # 5. Crear y entrenar Regresi√≥n Log√≠stica
        logger.info("="*50)
        lr_model, lr_info = create_logistic_regression_model(X_train, y_train, class_weights)
        lr_info['class_weights'] = class_weights  # A√±adir para el informe
        
        # Validar Regresi√≥n Log√≠stica
        lr_valid = validate_model_training(lr_model, X_train, y_train, "Regresi√≥n Log√≠stica")
        if not lr_valid:
            raise ValueError("Validaci√≥n de Regresi√≥n Log√≠stica fall√≥")
        
        # 6. Guardar modelos
        logger.info("="*50)
        saved_files = save_models(rf_model, rf_info, lr_model, lr_info, timestamp)
        
        # 7. Generar visualizaciones
        logger.info("Generando visualizaciones...")
        viz_files = {}
        
        # Feature importance para Random Forest
        rf_viz = generate_feature_importance_visualization(rf_info, timestamp)
        if rf_viz:
            viz_files['feature_importance'] = rf_viz
        
        # Coeficientes para Regresi√≥n Log√≠stica
        lr_viz = generate_coefficients_visualization(lr_info, timestamp)
        if lr_viz:
            viz_files['coefficients'] = lr_viz
        
        # 8. Generar informe completo
        logger.info("Generando informe completo...")
        report_content = generate_comprehensive_report(
            rf_info, lr_info, class_weights, ratio, saved_files, 
            viz_files, training_file, timestamp
        )
        
        # 9. Guardar informe
        output_files = save_report_and_config(report_content, timestamp)
        
        # 10. Resumen final
        logger.info("="*80)
        logger.info("PASO 10 COMPLETADO EXITOSAMENTE")
        logger.info("="*80)
        logger.info("RESUMEN DE MODELOS CREADOS:")
        logger.info("")
        
        # Informaci√≥n de Random Forest
        logger.info("üå≥ RANDOM FOREST:")
        logger.info(f"  ‚Ä¢ N¬∞ √°rboles: {rf_info['parameters']['n_estimators']}")
        logger.info(f"  ‚Ä¢ Profundidad m√°xima: {rf_info['parameters']['max_depth']}")
        logger.info(f"  ‚Ä¢ Class weight: {class_weights}")
        logger.info(f"  ‚Ä¢ Normalizaci√≥n: No aplicada")
        logger.info(f"  ‚Ä¢ Archivo: {saved_files['random_forest_model']}")
        
        # Top 3 caracter√≠sticas m√°s importantes
        top_features = rf_info['feature_importance'][:3]
        logger.info(f"  ‚Ä¢ Top 3 caracter√≠sticas:")
        for i, feat in enumerate(top_features, 1):
            logger.info(f"    {i}. {feat['feature']}: {feat['importance']:.4f}")
        logger.info("")
        
        # Informaci√≥n de Regresi√≥n Log√≠stica
        logger.info("üìà REGRESI√ìN LOG√çSTICA:")
        logger.info(f"  ‚Ä¢ Solver: {lr_info['parameters']['solver']}")
        logger.info(f"  ‚Ä¢ Max iteraciones: {lr_info['parameters']['max_iter']}")
        logger.info(f"  ‚Ä¢ Class weight: {class_weights}")
        logger.info(f"  ‚Ä¢ Normalizaci√≥n: StandardScaler aplicado")
        logger.info(f"  ‚Ä¢ Archivo: {saved_files['logistic_regression_model']}")
        
        # Top 3 coeficientes por magnitud
        top_coefs = lr_info['coefficients'][:3]
        logger.info(f"  ‚Ä¢ Top 3 coeficientes (magnitud):")
        for i, coef in enumerate(top_coefs, 1):
            direction = "‚Üë" if coef['coefficient'] > 0 else "‚Üì"
            logger.info(f"    {i}. {coef['feature']}: {coef['coefficient']:+.4f} {direction}")
        logger.info("")
        
        # Configuraci√≥n de datos
        logger.info("üìä CONFIGURACI√ìN DE ENTRENAMIENTO:")
        logger.info(f"  ‚Ä¢ Muestras de entrenamiento: {rf_info['training_samples']:,}")
        logger.info(f"  ‚Ä¢ N√∫mero de caracter√≠sticas: {rf_info['n_features']}")
        logger.info(f"  ‚Ä¢ Ratio de clases: {ratio:.2f}:1")
        logger.info(f"  ‚Ä¢ Tasa de churn: {y_train.mean():.1%}")
        logger.info("")
        
        logger.info("üìÅ ARCHIVOS GENERADOS:")
        logger.info(f"  ‚Ä¢ Modelos entrenados: {len([k for k in saved_files.keys() if 'model' in k])}")
        logger.info(f"  ‚Ä¢ Archivos de informaci√≥n: {len([k for k in saved_files.keys() if 'info' in k])}")
        logger.info(f"  ‚Ä¢ Visualizaciones: {len(viz_files)}")
        logger.info(f"  ‚Ä¢ Informe detallado: {output_files['report_file']}")
        logger.info("")
        
        logger.info("‚úÖ VALIDACIONES COMPLETADAS:")
        logger.info(f"  ‚Ä¢ Random Forest: {'‚úÖ Exitosa' if rf_valid else '‚ùå Fallida'}")
        logger.info(f"  ‚Ä¢ Regresi√≥n Log√≠stica: {'‚úÖ Exitosa' if lr_valid else '‚ùå Fallida'}")
        logger.info(f"  ‚Ä¢ Guardado de modelos: ‚úÖ Exitoso")
        logger.info(f"  ‚Ä¢ Generaci√≥n de visualizaciones: ‚úÖ Exitosa")
        logger.info("")
        
        logger.info("üöÄ MODELOS LISTOS PARA EVALUACI√ìN:")
        logger.info(f"  ‚Ä¢ Random Forest: modelos/random_forest_model_{timestamp}.pkl")
        logger.info(f"  ‚Ä¢ Regresi√≥n Log√≠stica: modelos/logistic_regression_pipeline_{timestamp}.pkl")
        logger.info("")
        
        logger.info("üìä PR√ìXIMO PASO SUGERIDO:")
        logger.info("  Paso 11: Evaluaci√≥n de Modelos")
        logger.info("  - Cargar modelos entrenados")
        logger.info("  - Evaluar en conjuntos de validaci√≥n y test")  
        logger.info("  - Comparar rendimiento con m√©tricas especializadas")
        logger.info("  - Generar matrices de confusi√≥n y curvas ROC/PR")
        logger.info("  - Seleccionar modelo final basado en performance")
        logger.info("="*80)
        
    except Exception as e:
        logger.error(f"ERROR EN EL PROCESO: {str(e)}")
        import traceback
        logger.error(f"Traceback completo: {traceback.format_exc()}")
        raise

if __name__ == "__main__":
    main()