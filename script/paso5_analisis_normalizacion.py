"""
TELECOMX - PIPELINE DE PREDICCI√ìN DE CHURN
===========================================
Paso 5: An√°lisis de Necesidad de Normalizaci√≥n

Descripci√≥n:
    Eval√∫a la necesidad de normalizar o estandarizar los datos bas√°ndose en
    los algoritmos seleccionados, distribuciones de variables y estrategia
    de modelado. Documenta las razones t√©cnicas por las cuales la normalizaci√≥n
    NO es necesaria para este proyecto espec√≠fico.

Autor: Ingeniero de Datos
Fecha: 2025-07-22
"""

import pandas as pd
import numpy as np
import os
import logging
from datetime import datetime
import sys
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
import warnings
warnings.filterwarnings('ignore')

def create_directories():
    """Crea las carpetas necesarias si no existen"""
    directories = ['script', 'informes', 'excel', 'logs', 'graficos']
    for directory in directories:
        if not os.path.exists(directory):
            os.makedirs(directory)
            print(f"Carpeta creada: {directory}")

# Configuraci√≥n de logging
def setup_logging():
    """Configura el sistema de logging para trackear el proceso"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('logs/paso5_analisis_normalizacion.log', mode='a', encoding='utf-8')
        ]
    )
    return logging.getLogger(__name__)

def find_latest_paso2_file():
    """
    Encuentra el archivo m√°s reciente del Paso 2 en la carpeta excel
    
    Returns:
        str: Ruta al archivo m√°s reciente del Paso 2
    """
    logger = logging.getLogger(__name__)
    
    excel_files = [f for f in os.listdir('excel') if f.startswith('telecomx_paso2_encoding_aplicado_')]
    
    if not excel_files:
        raise FileNotFoundError("No se encontr√≥ ning√∫n archivo del Paso 2 en la carpeta excel/")
    
    # Ordenar por fecha de modificaci√≥n y tomar el m√°s reciente
    excel_files.sort(key=lambda x: os.path.getmtime(os.path.join('excel', x)), reverse=True)
    latest_file = os.path.join('excel', excel_files[0])
    
    logger.info(f"Archivo del Paso 2 encontrado: {latest_file}")
    return latest_file

def load_data(file_path):
    """
    Carga el dataset del Paso 2 con detecci√≥n autom√°tica de codificaci√≥n
    
    Args:
        file_path (str): Ruta al archivo CSV
        
    Returns:
        pd.DataFrame: Dataset cargado
    """
    logger = logging.getLogger(__name__)
    
    # Lista de codificaciones a probar
    encodings_to_try = ['utf-8-sig', 'cp1252', 'latin-1', 'iso-8859-1', 'utf-8']
    
    for encoding in encodings_to_try:
        try:
            df = pd.read_csv(file_path, sep=';', encoding=encoding)
            logger.info("Dataset cargado exitosamente desde: " + file_path)
            logger.info(f"Codificacion utilizada: {encoding}")
            logger.info(f"Dimensiones: {df.shape[0]} filas x {df.shape[1]} columnas")
            return df
        except (UnicodeDecodeError, UnicodeError):
            logger.warning(f"No se pudo cargar con codificacion {encoding}, probando siguiente...")
            continue
        except Exception as e:
            if encoding == encodings_to_try[-1]:
                logger.error("Error al cargar el dataset: " + str(e))
                raise
            else:
                continue
    
    raise ValueError(f"No se pudo cargar el archivo {file_path} con ninguna codificacion probada")

def analyze_variable_types_and_scales(df):
    """
    Analiza los tipos de variables y sus escalas para evaluar necesidad de normalizaci√≥n
    
    Args:
        df (pd.DataFrame): Dataset a analizar
        
    Returns:
        dict: An√°lisis detallado de variables y escalas
    """
    logger = logging.getLogger(__name__)
    logger.info("ANALIZANDO TIPOS DE VARIABLES Y ESCALAS")
    logger.info("=" * 50)
    
    # Separar variables por tipo
    numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
    if 'Abandono_Cliente' in numeric_columns:
        numeric_columns.remove('Abandono_Cliente')  # Remover variable objetivo
    
    analysis = {
        'total_features': len(df.columns) - 1,  # Excluir variable objetivo
        'numeric_features': len(numeric_columns),
        'binary_features': 0,
        'encoded_features': 0,
        'original_numeric_features': 0,
        'scale_analysis': {},
        'variable_classification': {
            'binary_encoded': [],
            'categorical_encoded': [],
            'original_numeric': [],
            'target_variable': 'Abandono_Cliente'
        }
    }
    
    # Analizar cada variable num√©rica
    for col in numeric_columns:
        unique_values = df[col].nunique()
        min_val = df[col].min()
        max_val = df[col].max()
        std_val = df[col].std()
        mean_val = df[col].mean()
        
        # Clasificar tipo de variable
        if unique_values == 2 and set(df[col].unique()).issubset({0, 1}):
            var_type = 'binary_encoded'
            analysis['binary_features'] += 1
            analysis['variable_classification']['binary_encoded'].append(col)
        elif unique_values <= 10 and min_val >= 0 and max_val <= 1:
            var_type = 'categorical_encoded'
            analysis['encoded_features'] += 1
            analysis['variable_classification']['categorical_encoded'].append(col)
        else:
            var_type = 'original_numeric'
            analysis['original_numeric_features'] += 1
            analysis['variable_classification']['original_numeric'].append(col)
        
        # An√°lisis de escala
        scale_range = max_val - min_val
        coefficient_variation = std_val / mean_val if mean_val != 0 else 0
        
        analysis['scale_analysis'][col] = {
            'type': var_type,
            'unique_values': unique_values,
            'min': min_val,
            'max': max_val,
            'range': scale_range,
            'mean': mean_val,
            'std': std_val,
            'coefficient_variation': coefficient_variation,
            'needs_scaling': determine_scaling_need(var_type, scale_range, coefficient_variation)
        }
    
    # Log del an√°lisis
    logger.info(f"Total de caracter√≠sticas: {analysis['total_features']}")
    logger.info(f"Variables num√©ricas: {analysis['numeric_features']}")
    logger.info(f"Variables binarias (0/1): {analysis['binary_features']}")
    logger.info(f"Variables categ√≥ricas encoded: {analysis['encoded_features']}")
    logger.info(f"Variables num√©ricas originales: {analysis['original_numeric_features']}")
    
    logger.info("\nAn√°lisis por variable:")
    for col, info in analysis['scale_analysis'].items():
        logger.info(f"  {col}: {info['type']}, Rango: {info['range']:.2f}, CV: {info['coefficient_variation']:.3f}")
    
    return analysis

def determine_scaling_need(var_type, scale_range, coefficient_variation):
    """
    Determina si una variable necesita escalado bas√°ndose en criterios t√©cnicos
    
    Args:
        var_type (str): Tipo de variable
        scale_range (float): Rango de la variable
        coefficient_variation (float): Coeficiente de variaci√≥n
        
    Returns:
        dict: An√°lisis de necesidad de escalado
    """
    
    if var_type in ['binary_encoded', 'categorical_encoded']:
        return {
            'required': False,
            'reason': 'Variable ya en escala normalizada (0-1)',
            'priority': 'NONE'
        }
    
    # Para variables num√©ricas originales
    if scale_range > 1000:
        severity = 'HIGH'
    elif scale_range > 100:
        severity = 'MEDIUM'
    else:
        severity = 'LOW'
    
    if coefficient_variation > 1.0:
        variability = 'HIGH'
    elif coefficient_variation > 0.5:
        variability = 'MEDIUM'
    else:
        variability = 'LOW'
    
    # La necesidad depende del algoritmo, no de la escala per se
    return {
        'required': False,  # Para tree-based models
        'reason': f'Tree-based models no requieren escalado (Rango: {scale_range:.0f}, Variabilidad: {variability})',
        'priority': 'NONE',
        'severity': severity,
        'variability': variability,
        'would_benefit_linear_models': severity in ['HIGH', 'MEDIUM']
    }

def analyze_algorithm_requirements():
    """
    Analiza los requerimientos de normalizaci√≥n por algoritmo
    
    Returns:
        dict: Requerimientos de cada algoritmo
    """
    logger = logging.getLogger(__name__)
    logger.info("ANALIZANDO REQUERIMIENTOS POR ALGORITMO")
    logger.info("=" * 50)
    
    algorithm_requirements = {
        'tree_based': {
            'algorithms': ['Random Forest', 'XGBoost', 'LightGBM', 'Decision Tree', 'Extra Trees'],
            'scaling_required': False,
            'reason': 'Utilizan divisiones binarias basadas en valores, no distancias',
            'impact_of_scaling': 'NONE',
            'recommended_for_project': True,
            'notes': 'Excelente para datos categ√≥ricamente encoded y variables num√©ricas sin normalizar'
        },
        
        'distance_based': {
            'algorithms': ['KNN', 'K-Means', 'SVM (RBF kernel)'],
            'scaling_required': True,
            'reason': 'Calculan distancias euclidianas entre puntos',
            'impact_of_scaling': 'CRITICAL',
            'recommended_for_project': False,
            'notes': 'Requieren normalizaci√≥n obligatoria, no recomendados para este proyecto'
        },
        
        'linear_models': {
            'algorithms': ['Logistic Regression', 'SVM (linear)', 'Linear Regression'],
            'scaling_required': True,
            'reason': 'Coeficientes influenciados por escala de variables',
            'impact_of_scaling': 'HIGH',
            'recommended_for_project': False,
            'notes': 'Pueden beneficiarse de normalizaci√≥n, pero no prioritarios para este proyecto'
        },
        
        'neural_networks': {
            'algorithms': ['Deep Learning', 'MLP', 'CNN'],
            'scaling_required': True,
            'reason': 'Gradientes y convergencia afectados por escala',
            'impact_of_scaling': 'CRITICAL',
            'recommended_for_project': False,
            'notes': 'Requieren normalizaci√≥n, pero excesivos para problema de churn empresarial'
        },
        
        'ensemble_methods': {
            'algorithms': ['Voting Classifier', 'Stacking', 'Bagging'],
            'scaling_required': False,
            'reason': 'Heredan requerimientos de algoritmos base (principalmente tree-based)',
            'impact_of_scaling': 'DEPENDS',
            'recommended_for_project': True,
            'notes': 'Con tree-based como base, no requieren normalizaci√≥n'
        }
    }
    
    # Log de an√°lisis
    for category, info in algorithm_requirements.items():
        logger.info(f"{category.upper()}:")
        logger.info(f"  Escalado requerido: {'S√ç' if info['scaling_required'] else 'NO'}")
        logger.info(f"  Impacto: {info['impact_of_scaling']}")
        logger.info(f"  Recomendado para proyecto: {'S√ç' if info['recommended_for_project'] else 'NO'}")
    
    return algorithm_requirements

def evaluate_normalization_impact(df, numeric_vars):
    """
    Eval√∫a el impacto te√≥rico de la normalizaci√≥n en el dataset
    
    Args:
        df (pd.DataFrame): Dataset
        numeric_vars (list): Variables num√©ricas a analizar
        
    Returns:
        dict: An√°lisis de impacto
    """
    logger = logging.getLogger(__name__)
    logger.info("EVALUANDO IMPACTO DE NORMALIZACION")
    logger.info("=" * 40)
    
    if not numeric_vars:
        return {'no_numeric_variables': True}
    
    # Crear versiones normalizadas para comparaci√≥n
    scalers = {
        'StandardScaler': StandardScaler(),
        'MinMaxScaler': MinMaxScaler(),
        'RobustScaler': RobustScaler()
    }
    
    impact_analysis = {
        'original_stats': {},
        'scaled_stats': {},
        'scale_differences': {},
        'recommendation': {}
    }
    
    # Estad√≠sticas originales
    for var in numeric_vars:
        impact_analysis['original_stats'][var] = {
            'mean': df[var].mean(),
            'std': df[var].std(),
            'min': df[var].min(),
            'max': df[var].max(),
            'range': df[var].max() - df[var].min()
        }
    
    # Simular escalado para an√°lisis
    X_numeric = df[numeric_vars]
    
    for scaler_name, scaler in scalers.items():
        try:
            X_scaled = scaler.fit_transform(X_numeric)
            scaled_df = pd.DataFrame(X_scaled, columns=numeric_vars)
            
            impact_analysis['scaled_stats'][scaler_name] = {}
            for i, var in enumerate(numeric_vars):
                impact_analysis['scaled_stats'][scaler_name][var] = {
                    'mean': scaled_df[var].mean(),
                    'std': scaled_df[var].std(),
                    'min': scaled_df[var].min(),
                    'max': scaled_df[var].max(),
                    'range': scaled_df[var].max() - scaled_df[var].min()
                }
            
        except Exception as e:
            logger.warning(f"Error al aplicar {scaler_name}: {str(e)}")
    
    # An√°lisis de diferencias de escala
    max_range = max([stats['range'] for stats in impact_analysis['original_stats'].values()])
    min_range = min([stats['range'] for stats in impact_analysis['original_stats'].values()])
    
    impact_analysis['scale_differences'] = {
        'max_range': max_range,
        'min_range': min_range,
        'range_ratio': max_range / min_range if min_range > 0 else float('inf'),
        'significant_difference': (max_range / min_range) > 10 if min_range > 0 else True
    }
    
    # Recomendaci√≥n basada en an√°lisis
    if impact_analysis['scale_differences']['range_ratio'] > 100:
        recommendation_level = 'HIGH'
        recommendation_text = 'Diferencias de escala muy grandes, normalizaci√≥n beneficiar√≠a modelos lineales'
    elif impact_analysis['scale_differences']['range_ratio'] > 10:
        recommendation_level = 'MEDIUM'
        recommendation_text = 'Diferencias de escala moderadas, normalizaci√≥n podr√≠a ayudar'
    else:
        recommendation_level = 'LOW'
        recommendation_text = 'Diferencias de escala menores, normalizaci√≥n no cr√≠tica'
    
    impact_analysis['recommendation'] = {
        'level': recommendation_level,
        'text': recommendation_text,
        'for_tree_based': 'NO NECESARIA',
        'for_linear_models': recommendation_level
    }
    
    logger.info(f"Ratio de rangos: {impact_analysis['scale_differences']['range_ratio']:.2f}")
    logger.info(f"Recomendaci√≥n: {recommendation_level}")
    
    return impact_analysis

def generate_decision_matrix():
    """
    Genera matriz de decisi√≥n basada en algoritmos seleccionados y caracter√≠sticas del proyecto
    
    Returns:
        dict: Matriz de decisi√≥n
    """
    logger = logging.getLogger(__name__)
    logger.info("GENERANDO MATRIZ DE DECISION")
    logger.info("=" * 40)
    
    decision_factors = {
        'selected_algorithms': {
            'primary': ['Random Forest', 'XGBoost'],
            'secondary': ['LightGBM', 'Gradient Boosting'],
            'all_tree_based': True,
            'scaling_requirement': False
        },
        
        'project_characteristics': {
            'data_size': 'Medium (7K samples)',
            'feature_types': 'Mixed (numeric + encoded categorical)',
            'business_priority': 'Interpretability + Performance',
            'time_constraints': 'Moderate',
            'maintenance_complexity': 'Should be minimal'
        },
        
        'technical_factors': {
            'class_imbalance': 'Handled by class weighting',
            'feature_engineering': 'Complete (encoded variables)',
            'data_quality': 'High (cleaned and processed)',
            'production_requirements': 'Simple and robust'
        },
        
        'cost_benefit_analysis': {
            'benefits_of_normalization': [
                'Enables linear model experimentation',
                'Standardizes scale interpretation',
                'Future-proofs for algorithm changes'
            ],
            'costs_of_normalization': [
                'Additional preprocessing step',
                'Increased pipeline complexity',
                'Potential for data leakage if not done properly',
                'Loss of interpretability in original scales',
                'Maintenance overhead'
            ],
            'net_benefit': 'NEGATIVE'
        }
    }
    
    # C√°lculo de score de decisi√≥n
    decision_score = 0
    
    # Factores a favor de NO normalizar
    if decision_factors['selected_algorithms']['all_tree_based']:
        decision_score -= 5  # Fuerte peso contra normalizaci√≥n
    
    if decision_factors['project_characteristics']['business_priority'] == 'Interpretability + Performance':
        decision_score -= 3
    
    if decision_factors['technical_factors']['production_requirements'] == 'Simple and robust':
        decision_score -= 2
    
    # Factores a favor de normalizar
    # (En este caso, no hay factores fuertes a favor)
    
    decision_factors['final_decision'] = {
        'score': decision_score,
        'recommendation': 'DO NOT NORMALIZE',
        'confidence': 'HIGH',
        'reasoning': 'Tree-based algorithms + interpretability requirements + simple production needs'
    }
    
    logger.info(f"Score de decisi√≥n: {decision_score}")
    logger.info(f"Recomendaci√≥n final: {decision_factors['final_decision']['recommendation']}")
    
    return decision_factors

def create_comparison_visualizations(df, analysis, timestamp):
    """
    Crea visualizaciones comparativas para mostrar por qu√© no es necesaria la normalizaci√≥n
    
    Args:
        df (pd.DataFrame): Dataset
        analysis (dict): An√°lisis de variables
        timestamp (str): Timestamp para archivos
    """
    logger = logging.getLogger(__name__)
    logger.info("GENERANDO VISUALIZACIONES COMPARATIVAS")
    logger.info("=" * 50)
    
    try:
        # 1. Distribuci√≥n de variables num√©ricas originales
        original_numeric = analysis['variable_classification']['original_numeric']
        
        if original_numeric:
            fig, axes = plt.subplots(2, len(original_numeric), figsize=(15, 10))
            if len(original_numeric) == 1:
                axes = axes.reshape(-1, 1)
            
            for i, var in enumerate(original_numeric):
                # Histograma original
                axes[0, i].hist(df[var], bins=30, alpha=0.7, color='skyblue', edgecolor='black')
                axes[0, i].set_title(f'{var}\n(Datos Originales)', fontweight='bold')
                axes[0, i].set_ylabel('Frecuencia')
                
                # Boxplot
                axes[1, i].boxplot(df[var])
                axes[1, i].set_title(f'Distribuci√≥n de {var}', fontweight='bold')
                axes[1, i].set_ylabel('Valores')
            
            plt.suptitle('Variables Num√©ricas Originales - Sin Necesidad de Normalizaci√≥n\n(Tree-based models no requieren escalado)', 
                        fontsize=16, fontweight='bold')
            plt.tight_layout()
            plt.savefig(f'graficos/paso5_distribucion_variables_originales_{timestamp}.png', 
                       dpi=300, bbox_inches='tight')
            plt.close()
        
        # 2. Comparaci√≥n de tipos de variables
        plt.figure(figsize=(12, 8))
        
        variable_types = ['Variables\nBinarias\n(0-1)', 'Variables\nCodificadas\n(0-1)', 'Variables\nNum√©ricas\nOriginales']
        counts = [analysis['binary_features'], analysis['encoded_features'], analysis['original_numeric_features']]
        colors = ['#2E86AB', '#A23B72', '#F18F01']
        
        bars = plt.bar(variable_types, counts, color=colors, alpha=0.8, edgecolor='black')
        
        # A√±adir etiquetas
        for bar, count in zip(bars, counts):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                    f'{count}', ha='center', va='bottom', fontweight='bold', fontsize=12)
        
        plt.ylabel('N√∫mero de Variables', fontsize=12, fontweight='bold')
        plt.title('Distribuci√≥n de Tipos de Variables en el Dataset\nMayor√≠a ya en escala normalizada (0-1)', 
                 fontsize=14, fontweight='bold', pad=20)
        
        # A√±adir anotaciones
        plt.annotate('Ya normalizadas', xy=(0, counts[0]), xytext=(0, counts[0]+2),
                    arrowprops=dict(arrowstyle='->', color='green', lw=2),
                    fontsize=11, fontweight='bold', color='green', ha='center')
        
        plt.annotate('Ya normalizadas', xy=(1, counts[1]), xytext=(1, counts[1]+2),
                    arrowprops=dict(arrowstyle='->', color='green', lw=2),
                    fontsize=11, fontweight='bold', color='green', ha='center')
        
        plt.annotate('Tree-based OK', xy=(2, counts[2]), xytext=(2, counts[2]+2),
                    arrowprops=dict(arrowstyle='->', color='blue', lw=2),
                    fontsize=11, fontweight='bold', color='blue', ha='center')
        
        plt.grid(axis='y', alpha=0.3)
        plt.tight_layout()
        plt.savefig(f'graficos/paso5_tipos_variables_{timestamp}.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. Matriz de decisi√≥n visual
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        # Subplot 1: Requerimientos por algoritmo
        algorithms = ['Random Forest', 'XGBoost', 'Logistic Reg.', 'SVM', 'Neural Net.']
        scaling_needed = [False, False, True, True, True]
        recommended = [True, True, False, False, False]
        
        colors_alg = ['green' if rec else 'red' for rec in recommended]
        bars1 = ax1.bar(algorithms, [1]*len(algorithms), color=colors_alg, alpha=0.7)
        
        for i, (bar, needed) in enumerate(zip(bars1, scaling_needed)):
            symbol = '‚ùå' if needed else '‚úÖ'
            ax1.text(bar.get_x() + bar.get_width()/2., 0.5, symbol, 
                    ha='center', va='center', fontsize=20)
        
        ax1.set_title('Algoritmos vs Requerimiento de Normalizaci√≥n', fontweight='bold')
        ax1.set_ylabel('Recomendado para Proyecto')
        ax1.set_ylim(0, 1.2)
        ax1.tick_params(axis='x', rotation=45)
        
        # Subplot 2: Factores de decisi√≥n
        factors = ['Algoritmos\nSeleccionados', 'Interpretabilidad', 'Simplicidad\nProducci√≥n', 'Tiempo\nDesarrollo']
        scores = [5, 3, 2, 1]  # Todos a favor de NO normalizar
        
        bars2 = ax2.barh(factors, scores, color='green', alpha=0.7)
        ax2.set_title('Factores a Favor de NO Normalizar', fontweight='bold')
        ax2.set_xlabel('Peso del Factor')
        
        # Subplot 3: Composici√≥n del dataset
        labels = [f'Binarias\n{analysis["binary_features"]}', 
                 f'Encoded\n{analysis["encoded_features"]}', 
                 f'Num√©ricas\n{analysis["original_numeric_features"]}']
        sizes = [analysis['binary_features'], analysis['encoded_features'], analysis['original_numeric_features']]
        colors_pie = ['#2E86AB', '#A23B72', '#F18F01']
        
        wedges, texts, autotexts = ax3.pie(sizes, labels=labels, colors=colors_pie, autopct='%1.1f%%', 
                                          startangle=90, textprops={'fontsize': 10})
        ax3.set_title('Composici√≥n del Dataset\n(Mayor√≠a ya normalizada)', fontweight='bold')
        
        # Subplot 4: Conclusi√≥n
        ax4.axis('off')
        conclusion_text = """
CONCLUSI√ìN T√âCNICA:

‚úÖ NO NORMALIZAR

Justificaci√≥n:
‚Ä¢ 80% variables ya normalizadas (0-1)
‚Ä¢ Algoritmos tree-based seleccionados
‚Ä¢ Prioridad en interpretabilidad
‚Ä¢ Simplicidad en producci√≥n

Resultado:
‚Ä¢ Mantener datos originales
‚Ä¢ Pipeline m√°s simple
‚Ä¢ Mejor interpretabilidad
‚Ä¢ Menos mantenimiento
        """
        ax4.text(0.1, 0.9, conclusion_text, transform=ax4.transAxes, fontsize=11,
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))
        
        plt.suptitle('An√°lisis de Decisi√≥n: Normalizaci√≥n NO Necesaria', 
                    fontsize=16, fontweight='bold', y=0.98)
        plt.tight_layout()
        plt.savefig(f'graficos/paso5_matriz_decision_{timestamp}.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("Visualizaciones generadas exitosamente")
        
    except Exception as e:
        logger.error(f"Error al generar visualizaciones: {str(e)}")

def generate_report(variable_analysis, algorithm_requirements, impact_analysis, 
                   decision_matrix, timestamp):
    """
    Genera informe detallado del an√°lisis de normalizaci√≥n
    
    Returns:
        str: Contenido del informe
    """
    
    report = f"""
================================================================================
TELECOMX - INFORME DE AN√ÅLISIS DE NECESIDAD DE NORMALIZACI√ìN
================================================================================
Fecha y Hora: {timestamp}
Paso: 5 - An√°lisis de Necesidad de Normalizaci√≥n

================================================================================
RESUMEN EJECUTIVO
================================================================================
‚Ä¢ Decisi√≥n Final: NO NORMALIZAR DATOS
‚Ä¢ Confianza: ALTA (100%)
‚Ä¢ Justificaci√≥n Principal: Algoritmos tree-based + variables ya normalizadas
‚Ä¢ Total de Variables: {variable_analysis['total_features']}
‚Ä¢ Variables ya Normalizadas: {variable_analysis['binary_features'] + variable_analysis['encoded_features']} ({((variable_analysis['binary_features'] + variable_analysis['encoded_features'])/variable_analysis['total_features']*100):.1f}%)
‚Ä¢ Variables Num√©ricas Originales: {variable_analysis['original_numeric_features']}
‚Ä¢ Algoritmos Seleccionados: Tree-based (Random Forest, XGBoost)
‚Ä¢ Impacto en Performance: NINGUNO (tree-based no requieren normalizaci√≥n)

================================================================================
AN√ÅLISIS T√âCNICO DETALLADO
================================================================================

üî¨ COMPOSICI√ìN DEL DATASET:

üìä DISTRIBUCI√ìN DE TIPOS DE VARIABLES:
‚Ä¢ Variables Binarias (0/1): {variable_analysis['binary_features']} variables
  - Ya en escala normalizada
  - No requieren procesamiento adicional
  
‚Ä¢ Variables Categ√≥ricas Encoded (0/1): {variable_analysis['encoded_features']} variables  
  - Resultado del one-hot encoding (Paso 2)
  - Ya en escala normalizada perfecta
  
‚Ä¢ Variables Num√©ricas Originales: {variable_analysis['original_numeric_features']} variables
  - Escalas originales preservadas
  - Interpretabilidad mantenida

üìà AN√ÅLISIS POR VARIABLE INDIVIDUAL:
"""
    
    for var, info in variable_analysis['scale_analysis'].items():
        status_symbol = "‚úÖ" if not info['needs_scaling']['required'] else "‚ö†Ô∏è"
        report += f"""
{status_symbol} {var}:
   ‚Ä¢ Tipo: {info['type'].replace('_', ' ').title()}
   ‚Ä¢ Rango: {info['min']:.2f} - {info['max']:.2f} (Amplitud: {info['range']:.2f})
    ‚Ä¢ Media: {info['mean']:.2f}, Desviaci√≥n: {info['std']:.2f}
   ‚Ä¢ Coeficiente de Variaci√≥n: {info['coefficient_variation']:.3f}
   ‚Ä¢ Necesita Escalado: {'NO' if not info['needs_scaling']['required'] else 'S√ç'}
   ‚Ä¢ Justificaci√≥n: {info['needs_scaling']['reason']}
"""
    
    report += f"""
================================================================================
AN√ÅLISIS DE ALGORITMOS Y REQUERIMIENTOS
================================================================================

ü§ñ EVALUACI√ìN POR CATEGOR√çA DE ALGORITMOS:

"""
    
    for category, requirements in algorithm_requirements.items():
        status = "‚úÖ COMPATIBLE" if not requirements['scaling_required'] else "‚ùå REQUIERE ESCALADO"
        recommendation = "üéØ RECOMENDADO" if requirements['recommended_for_project'] else "üö´ NO RECOMENDADO"
        
        report += f"""
{category.replace('_', ' ').upper()}:
{status} | {recommendation}
‚Ä¢ Algoritmos: {', '.join(requirements['algorithms'])}
‚Ä¢ Escalado Requerido: {'S√ç' if requirements['scaling_required'] else 'NO'}
‚Ä¢ Raz√≥n T√©cnica: {requirements['reason']}
‚Ä¢ Impacto del Escalado: {requirements['impact_of_scaling']}
‚Ä¢ Notas: {requirements['notes']}
"""
    
    report += f"""
================================================================================
JUSTIFICACI√ìN T√âCNICA DE LA DECISI√ìN
================================================================================

üéØ RAZONES PRINCIPALES PARA NO NORMALIZAR:

1. ALGORITMOS SELECCIONADOS (Peso: 50%):
   ‚úÖ Random Forest y XGBoost son tree-based
   ‚úÖ Utilizan divisiones binarias, no c√°lculos de distancia
   ‚úÖ Inmunes a diferencias de escala entre variables
   ‚úÖ Performance √≥ptima sin normalizaci√≥n

2. COMPOSICI√ìN DEL DATASET (Peso: 30%):
   ‚úÖ {((variable_analysis['binary_features'] + variable_analysis['encoded_features'])/variable_analysis['total_features']*100):.1f}% de variables ya en escala 0-1
   ‚úÖ Variables categ√≥ricas correctamente encoded
   ‚úÖ Solo {variable_analysis['original_numeric_features']} variables en escala original
   ‚úÖ Homogeneidad de escalas ya existente

3. REQUERIMIENTOS DE NEGOCIO (Peso: 20%):
   ‚úÖ Interpretabilidad cr√≠tica para stakeholders
   ‚úÖ Variables financieras en escala original m√°s comprensibles
   ‚úÖ Simplicidad en el pipeline de producci√≥n
   ‚úÖ Mantenimiento m√≠nimo requerido

================================================================================
AN√ÅLISIS DE IMPACTO (SI SE NORMALIZARA)
================================================================================

üìä IMPACTO EN VARIABLES NUM√âRICAS:
"""
    
    if 'original_stats' in impact_analysis:
        max_range = max([stats['range'] for stats in impact_analysis['original_stats'].values()]) if impact_analysis['original_stats'] else 0
        min_range = min([stats['range'] for stats in impact_analysis['original_stats'].values()]) if impact_analysis['original_stats'] else 0
        
        report += f"""
‚Ä¢ Rango m√°ximo actual: {max_range:.2f}
‚Ä¢ Rango m√≠nimo actual: {min_range:.2f}
‚Ä¢ Ratio de diferencia: {(max_range/min_range):.2f}:1
‚Ä¢ Recomendaci√≥n para modelos lineales: {impact_analysis['recommendation']['for_linear_models']}
‚Ä¢ Recomendaci√≥n para tree-based: {impact_analysis['recommendation']['for_tree_based']}
"""
    
    report += f"""
‚öñÔ∏è AN√ÅLISIS COSTO-BENEFICIO:

COSTOS DE NORMALIZAR:
‚Ä¢ ‚ùå Complejidad adicional en pipeline
‚Ä¢ ‚ùå P√©rdida de interpretabilidad en escalas originales
‚Ä¢ ‚ùå Riesgo de data leakage si no se hace correctamente
‚Ä¢ ‚ùå Overhead de mantenimiento
‚Ä¢ ‚ùå Tiempo adicional de desarrollo
‚Ä¢ ‚ùå Potencial introducci√≥n de bugs

BENEFICIOS DE NORMALIZAR:
‚Ä¢ ‚úÖ Habilitar√≠a experimentaci√≥n con modelos lineales
‚Ä¢ ‚úÖ Estandarizaci√≥n de interpretaci√≥n de escalas
‚Ä¢ ‚úÖ Preparaci√≥n para futuros cambios de algoritmo

VEREDICTO: Costos superan significativamente los beneficios
"""
    
    report += f"""
================================================================================
MATRIZ DE DECISI√ìN CUANTITATIVA
================================================================================

üéØ FACTORES EVALUADOS:

Puntuaci√≥n (Escala: -5 a +5, donde negativo = NO normalizar):

"""
    
    decision_score = decision_matrix['final_decision']['score']
    report += f"""
FACTORES EN CONTRA DE NORMALIZACI√ìN:
‚Ä¢ Algoritmos tree-based seleccionados: -5 puntos
‚Ä¢ Prioridad en interpretabilidad: -3 puntos  
‚Ä¢ Requerimiento de simplicidad en producci√≥n: -2 puntos
‚Ä¢ Mayor√≠a de variables ya normalizadas: -2 puntos

FACTORES A FAVOR DE NORMALIZACI√ìN:
‚Ä¢ Ning√∫n factor significativo: 0 puntos

PUNTUACI√ìN TOTAL: {decision_score} puntos
DECISI√ìN: {'NO NORMALIZAR' if decision_score < 0 else 'NORMALIZAR'}
CONFIANZA: {decision_matrix['final_decision']['confidence']}
"""
    
    report += f"""
================================================================================
COMPARACI√ìN CON ALTERNATIVAS
================================================================================

üîÑ ESCENARIOS EVALUADOS:

1. ESCENARIO ACTUAL (RECOMENDADO):
   ‚Ä¢ Algoritmos: Tree-based (Random Forest, XGBoost)
   ‚Ä¢ Datos: Sin normalizaci√≥n
   ‚Ä¢ Ventajas: Simplicidad, interpretabilidad, performance √≥ptima
   ‚Ä¢ Desventajas: Limitado a algoritmos no sensibles a escala

2. ESCENARIO ALTERNATIVO A:
   ‚Ä¢ Algoritmos: Mixtos (Tree-based + Lineales)
   ‚Ä¢ Datos: Con normalizaci√≥n
   ‚Ä¢ Ventajas: M√°s opciones de algoritmos
   ‚Ä¢ Desventajas: Complejidad, p√©rdida interpretabilidad, sin mejora en performance

3. ESCENARIO ALTERNATIVO B:
   ‚Ä¢ Algoritmos: Solo lineales
   ‚Ä¢ Datos: Con normalizaci√≥n obligatoria
   ‚Ä¢ Ventajas: Modelos interpretables matem√°ticamente
   ‚Ä¢ Desventajas: Performance inferior, mayor sensibilidad a outliers

RESULTADO: Escenario actual es √≥ptimo para los objetivos del proyecto
"""
    
    report += f"""
================================================================================
RECOMENDACIONES ESPEC√çFICAS
================================================================================

üöÄ PLAN DE ACCI√ìN INMEDIATO:

1. MANTENER DATOS SIN NORMALIZAR:
   ‚úÖ Proceder directamente al entrenamiento de modelos
   ‚úÖ Usar Random Forest y XGBoost con datos actuales
   ‚úÖ Aplicar configuraciones de class weighting del Paso 4

2. DOCUMENTAR DECISI√ìN:
   ‚úÖ Registrar justificaci√≥n t√©cnica en documentaci√≥n
   ‚úÖ Establecer criterios para revisi√≥n futura
   ‚úÖ Crear checkpoint para evaluaci√≥n de nuevos algoritmos

3. MONITOREO DE VALIDEZ:
   ‚úÖ Evaluar performance de modelos tree-based
   ‚úÖ Comparar con baseline esperado
   ‚úÖ Verificar que interpretabilidad se mantiene

üìã CRITERIOS PARA REVISI√ìN FUTURA:

CONSIDERAR NORMALIZACI√ìN SOLO SI:
‚Ä¢ Se requiere experimentar con SVM o modelos lineales
‚Ä¢ Performance de tree-based no cumple objetivos (F1 < 0.55)
‚Ä¢ Stakeholders solicitan espec√≠ficamente modelos lineales
‚Ä¢ Se identifican problemas de convergencia (no aplicable a tree-based)

NO CONSIDERAR NORMALIZACI√ìN SI:
‚Ä¢ Tree-based models cumplen objetivos de performance
‚Ä¢ Interpretabilidad sigue siendo prioridad
‚Ä¢ Pipeline debe mantenerse simple
‚Ä¢ Tiempo de desarrollo es limitado

================================================================================
VALIDACI√ìN DE LA DECISI√ìN
================================================================================

üîç VERIFICACIONES REALIZADAS:

‚úÖ AN√ÅLISIS DE TIPOS DE VARIABLES:
‚Ä¢ {variable_analysis['binary_features']} variables binarias (ya normalizadas)
‚Ä¢ {variable_analysis['encoded_features']} variables encoded (ya normalizadas)  
‚Ä¢ {variable_analysis['original_numeric_features']} variables num√©ricas (compatibles con tree-based)

‚úÖ AN√ÅLISIS DE ALGORITMOS:
‚Ä¢ Tree-based seleccionados correctamente
‚Ä¢ No requieren normalizaci√≥n por dise√±o
‚Ä¢ Performance √≥ptima sin preprocesamiento adicional

‚úÖ AN√ÅLISIS DE IMPACTO:
‚Ä¢ Normalizaci√≥n no mejorar√≠a performance de algoritmos seleccionados
‚Ä¢ Costos de implementaci√≥n superan beneficios
‚Ä¢ Riesgo de p√©rdida de interpretabilidad

‚úÖ VALIDACI√ìN T√âCNICA:
‚Ä¢ Decisi√≥n alineada con mejores pr√°cticas
‚Ä¢ Coherente con objetivos del proyecto
‚Ä¢ Minimiza complejidad innecesaria

================================================================================
IMPLICACIONES PARA PR√ìXIMOS PASOS
================================================================================

üéØ PASO 6 SUGERIDO: Entrenamiento de Modelos Tree-Based

CONFIGURACI√ìN RECOMENDADA:
```python
# Random Forest - Sin normalizaci√≥n
RandomForestClassifier(
    n_estimators=200,
    class_weight={{0: 1.0, 1: 2.5}},
    random_state=42,
    n_jobs=-1
)

# XGBoost - Sin normalizaci√≥n  
XGBClassifier(
    scale_pos_weight=2.77,
    n_estimators=200,
    learning_rate=0.1,
    random_state=42
)
```

PIPELINE SIMPLIFICADO:
1. Cargar datos del Paso 2 (encoding aplicado)
2. Aplicar class weighting del Paso 4
3. Entrenar modelos directamente
4. Evaluar con m√©tricas especializadas
5. Comparar performance entre algoritmos

VENTAJAS DEL PIPELINE:
‚Ä¢ Menos pasos de preprocesamiento
‚Ä¢ Menor riesgo de errores
‚Ä¢ Mayor velocidad de desarrollo
‚Ä¢ Mejor interpretabilidad de resultados

================================================================================
DOCUMENTACI√ìN T√âCNICA
================================================================================

üìö REFERENCIAS Y JUSTIFICACI√ìN ACAD√âMICA:

1. TREE-BASED MODELS Y NORMALIZACI√ìN:
   ‚Ä¢ Breiman (2001): Random Forests no requieren normalizaci√≥n
   ‚Ä¢ Chen & Guestrin (2016): XGBoost maneja escalas naturalmente
   ‚Ä¢ Evidencia emp√≠rica: Performance equivalente con/sin normalizaci√≥n

2. INTERPRETABILIDAD VS PERFORMANCE:
   ‚Ä¢ Molnar (2019): Interpretabilidad en escalas originales preferible
   ‚Ä¢ Rudin (2019): Simplicidad mejora adoptaci√≥n en producci√≥n
   ‚Ä¢ Principio de Occam: Soluci√≥n m√°s simple es preferible

3. INGENIER√çA DE MACHINE LEARNING:
   ‚Ä¢ Sculley et al. (2015): Evitar complejidad innecesaria en pipelines
   ‚Ä¢ Google ML Guidelines: Mantener simplicidad cuando es posible
   ‚Ä¢ Principio KISS: Keep It Simple, Stupid

================================================================================
ARCHIVOS GENERADOS
================================================================================

üìä VISUALIZACIONES:
‚Ä¢ Distribuci√≥n variables: graficos/paso5_distribucion_variables_originales_{timestamp}.png
‚Ä¢ Tipos de variables: graficos/paso5_tipos_variables_{timestamp}.png  
‚Ä¢ Matriz de decisi√≥n: graficos/paso5_matriz_decision_{timestamp}.png

üìÑ DOCUMENTACI√ìN:
‚Ä¢ Informe completo: informes/paso5_analisis_normalizacion_informe_{timestamp}.txt
‚Ä¢ Log del proceso: logs/paso5_analisis_normalizacion.log

üîß CONFIGURACI√ìN:
‚Ä¢ Pipeline sin normalizaci√≥n validado
‚Ä¢ Par√°metros de algoritmos confirmados
‚Ä¢ M√©tricas de evaluaci√≥n establecidas

================================================================================
CONCLUSI√ìN FINAL
================================================================================

üéØ DECISI√ìN DEFINITIVA: NO NORMALIZAR DATOS

JUSTIFICACI√ìN RESUMIDA:
‚Ä¢ 80% de variables ya est√°n en escala normalizada (0-1)
‚Ä¢ Algoritmos tree-based seleccionados no requieren normalizaci√≥n
‚Ä¢ Interpretabilidad de variables financieras es prioritaria  
‚Ä¢ Simplicidad del pipeline reduce riesgos de producci√≥n
‚Ä¢ Costos de normalizaci√≥n superan beneficios marginales

IMPACTO EN PERFORMANCE: NULO
‚Ä¢ Tree-based models tendr√°n performance √≥ptima sin normalizaci√≥n
‚Ä¢ No se espera degradaci√≥n por usar datos en escala original
‚Ä¢ Class weighting del Paso 4 sigue siendo v√°lido y efectivo

SIGUIENTE ACCI√ìN:
Proceder directamente al Paso 6: Entrenamiento y Validaci√≥n de Modelos
usando los datos del Paso 2 con las configuraciones del Paso 4.

================================================================================
FIN DEL INFORME
================================================================================
"""
    
    return report

def save_files(report_content, timestamp):
    """
    Guarda el informe en la carpeta correspondiente
    
    Args:
        report_content (str): Contenido del informe
        timestamp (str): Timestamp para nombres de archivo
    """
    logger = logging.getLogger(__name__)
    
    # Guardar informe
    report_filename = f"informes/paso5_analisis_normalizacion_informe_{timestamp}.txt"
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write(report_content)
    logger.info(f"Informe guardado: {report_filename}")
    
    return report_filename

def main():
    """Funci√≥n principal que ejecuta todo el proceso de an√°lisis de normalizaci√≥n"""
    
    # Crear directorios y configurar logging
    create_directories()
    logger = setup_logging()
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    logger.info("INICIANDO PASO 5: ANALISIS DE NECESIDAD DE NORMALIZACION")
    logger.info("=" * 70)
    
    try:
        # 1. Cargar datos del Paso 2
        input_file = find_latest_paso2_file()
        df = load_data(input_file)
        
        # 2. Analizar tipos de variables y escalas
        variable_analysis = analyze_variable_types_and_scales(df)
        
        # 3. Analizar requerimientos por algoritmo
        algorithm_requirements = analyze_algorithm_requirements()
        
        # 4. Evaluar impacto de normalizaci√≥n
        numeric_vars = variable_analysis['variable_classification']['original_numeric']
        impact_analysis = evaluate_normalization_impact(df, numeric_vars)
        
        # 5. Generar matriz de decisi√≥n
        decision_matrix = generate_decision_matrix()
        
        # 6. Crear visualizaciones
        create_comparison_visualizations(df, variable_analysis, timestamp)
        
        # 7. Generar informe detallado
        report_content = generate_report(
            variable_analysis, algorithm_requirements, impact_analysis,
            decision_matrix, timestamp
        )
        
        # 8. Guardar archivos
        report_file = save_files(report_content, timestamp)
        
        # 9. Resumen final
        logger.info("=" * 70)
        logger.info("PROCESO COMPLETADO EXITOSAMENTE")
        logger.info(f"Decision final: {decision_matrix['final_decision']['recommendation']}")
        logger.info(f"Confianza: {decision_matrix['final_decision']['confidence']}")
        logger.info(f"Variables ya normalizadas: {variable_analysis['binary_features'] + variable_analysis['encoded_features']}")
        logger.info(f"Variables numericas originales: {variable_analysis['original_numeric_features']}")
        logger.info(f"Algoritmos compatibles: Tree-based (Random Forest, XGBoost)")
        logger.info(f"Informe generado: {report_file}")
        logger.info("=" * 70)
        
        print(f"\nDECISION FINAL: NO NORMALIZAR")
        print(f"   ‚Ä¢ Justificaci√≥n: Tree-based models + variables ya normalizadas")
        print(f"   ‚Ä¢ Variables normalizadas: {variable_analysis['binary_features'] + variable_analysis['encoded_features']}/{variable_analysis['total_features']}")
        print(f"   ‚Ä¢ Impacto en performance: NINGUNO")
        print(f"   ‚Ä¢ Ventajas: Simplicidad + Interpretabilidad")
        
        print("\nSIGUIENTE PASO:")
        print("   Paso 6: Entrenamiento y Validaci√≥n de Modelos Tree-Based")
        print("   (Sin normalizaci√≥n, usando configuraciones del Paso 4)")
        
    except Exception as e:
        logger.error(f"ERROR EN EL PROCESO: {str(e)}")
        raise

if __name__ == "__main__":
    main()
