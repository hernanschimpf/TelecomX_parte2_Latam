"""
TELECOMX - PIPELINE DE PREDICCI√ìN DE CHURN
===========================================
Paso 4: Configuraci√≥n de Class Weighting

Descripci√≥n:
    Establece configuraciones √≥ptimas de class weighting para manejar el 
    desbalance de clases (26.5% Churn, Ratio 2.77:1) sin modificar los datos 
    originales. Prepara el pipeline de evaluaci√≥n con m√©tricas especializadas 
    y configuraciones espec√≠ficas por algoritmo.

Autor: Ingeniero de Datos
Fecha: 2025-07-22
"""

import pandas as pd
import numpy as np
import os
import logging
from datetime import datetime
import sys
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import roc_auc_score, average_precision_score
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

def create_directories():
    """Crea las carpetas necesarias si no existen"""
    directories = ['script', 'informes', 'excel', 'logs', 'graficos']
    for directory in directories:
        if not os.path.exists(directory):
            os.makedirs(directory)
            print(f"Carpeta creada: {directory}")

# Configuraci√≥n de logging
def setup_logging():
    """Configura el sistema de logging para trackear el proceso"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('logs/paso4_class_weighting.log', mode='a', encoding='utf-8')
        ]
    )
    return logging.getLogger(__name__)

def find_latest_paso2_file():
    """
    Encuentra el archivo m√°s reciente del Paso 2 en la carpeta excel
    
    Returns:
        str: Ruta al archivo m√°s reciente del Paso 2
    """
    logger = logging.getLogger(__name__)
    
    excel_files = [f for f in os.listdir('excel') if f.startswith('telecomx_paso2_encoding_aplicado_')]
    
    if not excel_files:
        raise FileNotFoundError("No se encontr√≥ ning√∫n archivo del Paso 2 en la carpeta excel/")
    
    # Ordenar por fecha de modificaci√≥n y tomar el m√°s reciente
    excel_files.sort(key=lambda x: os.path.getmtime(os.path.join('excel', x)), reverse=True)
    latest_file = os.path.join('excel', excel_files[0])
    
    logger.info(f"Archivo del Paso 2 encontrado: {latest_file}")
    return latest_file

def load_data(file_path):
    """
    Carga el dataset del Paso 2 con detecci√≥n autom√°tica de codificaci√≥n
    
    Args:
        file_path (str): Ruta al archivo CSV
        
    Returns:
        pd.DataFrame: Dataset cargado
    """
    logger = logging.getLogger(__name__)
    
    # Lista de codificaciones a probar
    encodings_to_try = ['utf-8-sig', 'cp1252', 'latin-1', 'iso-8859-1', 'utf-8']
    
    for encoding in encodings_to_try:
        try:
            df = pd.read_csv(file_path, sep=';', encoding=encoding)
            logger.info("Dataset cargado exitosamente desde: " + file_path)
            logger.info(f"Codificacion utilizada: {encoding}")
            logger.info(f"Dimensiones: {df.shape[0]} filas x {df.shape[1]} columnas")
            return df
        except (UnicodeDecodeError, UnicodeError):
            logger.warning(f"No se pudo cargar con codificacion {encoding}, probando siguiente...")
            continue
        except Exception as e:
            if encoding == encodings_to_try[-1]:
                logger.error("Error al cargar el dataset: " + str(e))
                raise
            else:
                continue
    
    raise ValueError(f"No se pudo cargar el archivo {file_path} con ninguna codificacion probada")

def analyze_current_distribution(df):
    """
    Analiza la distribuci√≥n actual de clases
    
    Args:
        df (pd.DataFrame): Dataset con variable objetivo
        
    Returns:
        dict: Informaci√≥n de distribuci√≥n actual
    """
    logger = logging.getLogger(__name__)
    logger.info("ANALIZANDO DISTRIBUCION ACTUAL DE CLASES")
    logger.info("=" * 50)
    
    # Verificar variable objetivo
    if 'Abandono_Cliente' not in df.columns:
        raise ValueError("Variable objetivo 'Abandono_Cliente' no encontrada")
    
    # Calcular distribuci√≥n
    class_counts = df['Abandono_Cliente'].value_counts().sort_index()
    class_percentages = df['Abandono_Cliente'].value_counts(normalize=True).sort_index() * 100
    total_samples = len(df)
    
    distribution_info = {
        'total_samples': total_samples,
        'class_0_count': int(class_counts[0]),
        'class_1_count': int(class_counts[1]),
        'class_0_percentage': float(class_percentages[0]),
        'class_1_percentage': float(class_percentages[1]),
        'imbalance_ratio': float(class_counts[0] / class_counts[1]),
        'minority_class': 1 if class_counts[1] < class_counts[0] else 0,
        'majority_class': 0 if class_counts[1] < class_counts[0] else 1
    }
    
    logger.info(f"Total de muestras: {total_samples:,}")
    logger.info(f"Clase 0 (No Churn): {distribution_info['class_0_count']:,} ({distribution_info['class_0_percentage']:.1f}%)")
    logger.info(f"Clase 1 (Churn): {distribution_info['class_1_count']:,} ({distribution_info['class_1_percentage']:.1f}%)")
    logger.info(f"Ratio de desbalance: {distribution_info['imbalance_ratio']:.2f}:1")
    logger.info(f"Clase minoritaria: {distribution_info['minority_class']}")
    
    return distribution_info

def calculate_class_weights(distribution_info):
    """
    Calcula diferentes estrategias de class weighting
    
    Args:
        distribution_info (dict): Informaci√≥n de distribuci√≥n
        
    Returns:
        dict: Diferentes configuraciones de class weights
    """
    logger = logging.getLogger(__name__)
    logger.info("CALCULANDO ESTRATEGIAS DE CLASS WEIGHTING")
    logger.info("=" * 50)
    
    total_samples = distribution_info['total_samples']
    class_0_count = distribution_info['class_0_count']
    class_1_count = distribution_info['class_1_count']
    imbalance_ratio = distribution_info['imbalance_ratio']
    
    # Estrategia 1: Balanced (sklearn default)
    weight_balanced = {
        0: total_samples / (2 * class_0_count),
        1: total_samples / (2 * class_1_count)
    }
    
    # Estrategia 2: Inversely proportional
    weight_inverse = {
        0: 1.0,
        1: imbalance_ratio
    }
    
    # Estrategia 3: Square root (menos agresivo)
    sqrt_ratio = np.sqrt(imbalance_ratio)
    weight_sqrt = {
        0: 1.0,
        1: sqrt_ratio
    }
    
    # Estrategia 4: Custom conservative (recomendado para tu caso)
    # Para ratio 2.77:1, usar peso moderado
    conservative_weight = min(imbalance_ratio, 3.0)  # Cap m√°ximo de 3
    weight_conservative = {
        0: 1.0,
        1: conservative_weight
    }
    
    # Estrategia 5: Log-based (muy conservador)
    log_weight = 1 + np.log(imbalance_ratio)
    weight_log = {
        0: 1.0,
        1: log_weight
    }
    
    weighting_strategies = {
        'balanced': weight_balanced,
        'inverse': weight_inverse,
        'sqrt': weight_sqrt,
        'conservative': weight_conservative,
        'log': weight_log
    }
    
    # Logging de estrategias
    logger.info("Estrategias de class weighting calculadas:")
    for strategy_name, weights in weighting_strategies.items():
        ratio_str = f"{weights[1]:.2f}:1"
        logger.info(f"  {strategy_name}: Clase 0 = {weights[0]:.3f}, Clase 1 = {weights[1]:.3f} (Ratio: {ratio_str})")
    
    return weighting_strategies

def generate_algorithm_configurations(weighting_strategies, distribution_info):
    """
    Genera configuraciones espec√≠ficas para cada algoritmo
    
    Args:
        weighting_strategies (dict): Estrategias de weighting
        distribution_info (dict): Informaci√≥n de distribuci√≥n
        
    Returns:
        dict: Configuraciones por algoritmo
    """
    logger = logging.getLogger(__name__)
    logger.info("GENERANDO CONFIGURACIONES POR ALGORITMO")
    logger.info("=" * 50)
    
    imbalance_ratio = distribution_info['imbalance_ratio']
    
    algorithm_configs = {
        'random_forest': {
            'sklearn_params': {
                'conservative': {'class_weight': weighting_strategies['conservative']},
                'balanced': {'class_weight': 'balanced'},
                'custom': {'class_weight': {0: 1, 1: 2.5}}  # √ìptimo para tu ratio
            },
            'recommended': 'conservative',
            'notes': 'Random Forest maneja bien el desbalance. Conservative weighting recomendado.'
        },
        
        'xgboost': {
            'scale_pos_weight': {
                'conservative': weighting_strategies['conservative'][1],
                'balanced': imbalance_ratio,
                'sqrt': weighting_strategies['sqrt'][1]
            },
            'recommended': 'conservative',
            'notes': 'XGBoost usa scale_pos_weight. Valor 2.5-3.0 √≥ptimo para tu caso.'
        },
        
        'lightgbm': {
            'sklearn_params': {
                'conservative': {'class_weight': weighting_strategies['conservative']},
                'balanced': {'class_weight': 'balanced'},
                'is_unbalance': {'is_unbalance': True}
            },
            'recommended': 'conservative',
            'notes': 'LightGBM tiene par√°metro is_unbalance espec√≠fico para desbalance.'
        },
        
        'logistic_regression': {
            'sklearn_params': {
                'conservative': {'class_weight': weighting_strategies['conservative']},
                'balanced': {'class_weight': 'balanced'},
                'custom': {'class_weight': {0: 1, 1: 2.8}}
            },
            'recommended': 'balanced',
            'notes': 'Logistic Regression sensible a desbalance. Balanced weighting recomendado.'
        },
        
        'svm': {
            'sklearn_params': {
                'conservative': {'class_weight': weighting_strategies['conservative']},
                'balanced': {'class_weight': 'balanced'}
            },
            'recommended': 'balanced',
            'notes': 'SVM muy sensible a desbalance. Siempre usar class_weight.'
        },
        
        'gradient_boosting': {
            'sklearn_params': {
                'conservative': {'class_weight': weighting_strategies['conservative']},
                'custom': {'class_weight': {0: 1, 1: 2.5}}
            },
            'recommended': 'conservative',
            'notes': 'Gradient Boosting robusto. Conservative weighting suficiente.'
        }
    }
    
    # Log de configuraciones
    for algo_name, config in algorithm_configs.items():
        logger.info(f"{algo_name.upper()}:")
        logger.info(f"  Configuraci√≥n recomendada: {config['recommended']}")
        logger.info(f"  Notas: {config['notes']}")
    
    return algorithm_configs

def setup_evaluation_metrics():
    """
    Configura las m√©tricas de evaluaci√≥n apropiadas para datos desbalanceados
    
    Returns:
        dict: Configuraci√≥n de m√©tricas
    """
    logger = logging.getLogger(__name__)
    logger.info("CONFIGURANDO METRICAS DE EVALUACION")
    logger.info("=" * 40)
    
    metrics_config = {
        'primary_metrics': [
            'f1_score',
            'roc_auc',
            'average_precision',  # AUC-PR
            'balanced_accuracy'
        ],
        
        'secondary_metrics': [
            'precision',
            'recall',
            'accuracy'
        ],
        
        'business_metrics': [
            'precision_at_recall_70',  # Para capturar 70% de churns
            'recall_at_precision_80',  # Con 80% de precisi√≥n
            'false_positive_rate',
            'false_negative_rate'
        ],
        
        'metric_priorities': {
            'f1_score': 'HIGH',
            'average_precision': 'HIGH',  # M√°s importante que ROC-AUC para desbalance
            'recall': 'HIGH',  # Cr√≠tico para detectar churns
            'roc_auc': 'MEDIUM',
            'precision': 'MEDIUM',
            'accuracy': 'LOW'  # No confiable para datos desbalanceados
        },
        
        'target_thresholds': {
            'f1_score': 0.60,  # Objetivo m√≠nimo para tu ratio
            'average_precision': 0.65,
            'recall': 0.70,  # Capturar al menos 70% de churns
            'precision': 0.60,
            'roc_auc': 0.75
        }
    }
    
    logger.info("M√©tricas primarias configuradas:")
    for metric in metrics_config['primary_metrics']:
        priority = metrics_config['metric_priorities'].get(metric, 'MEDIUM')
        target = metrics_config['target_thresholds'].get(metric, 'N/A')
        logger.info(f"  {metric}: Prioridad {priority}, Objetivo {target}")
    
    return metrics_config

def create_evaluation_pipeline(df, weighting_strategies):
    """
    Crea pipeline de evaluaci√≥n con validaci√≥n cruzada estratificada
    
    Args:
        df (pd.DataFrame): Dataset completo
        weighting_strategies (dict): Estrategias de weighting
        
    Returns:
        dict: Pipeline de evaluaci√≥n configurado
    """
    logger = logging.getLogger(__name__)
    logger.info("CREANDO PIPELINE DE EVALUACION")
    logger.info("=" * 40)
    
    # Separar features y target
    X = df.drop('Abandono_Cliente', axis=1)
    y = df['Abandono_Cliente']
    
    # Configurar validaci√≥n cruzada estratificada
    cv_folds = 5
    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
    
    # Split inicial para hold-out test
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # Split train/validation del 80% restante
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp
    )
    
    pipeline_config = {
        'data_splits': {
            'X_train': X_train,
            'X_val': X_val, 
            'X_test': X_test,
            'y_train': y_train,
            'y_val': y_val,
            'y_test': y_test
        },
        
        'split_info': {
            'train_size': len(X_train),
            'val_size': len(X_val),
            'test_size': len(X_test),
            'train_churn_rate': y_train.mean(),
            'val_churn_rate': y_val.mean(),
            'test_churn_rate': y_test.mean()
        },
        
        'cv_strategy': {
            'method': 'StratifiedKFold',
            'n_splits': cv_folds,
            'shuffle': True,
            'random_state': 42
        },
        
        'sample_weights': {}
    }
    
    # Calcular sample weights para cada estrategia
    for strategy_name, class_weights in weighting_strategies.items():
        sample_weights = compute_sample_weight(class_weights, y_train)
        pipeline_config['sample_weights'][strategy_name] = sample_weights
    
    # Logging de informaci√≥n del split
    logger.info("Configuraci√≥n de splits:")
    logger.info(f"  Train: {len(X_train):,} muestras ({y_train.mean():.1%} churn)")
    logger.info(f"  Validation: {len(X_val):,} muestras ({y_val.mean():.1%} churn)")
    logger.info(f"  Test: {len(X_test):,} muestras ({y_test.mean():.1%} churn)")
    logger.info(f"  Validaci√≥n cruzada: {cv_folds} folds estratificados")
    
    return pipeline_config

def generate_baseline_evaluation():
    """
    Genera c√≥digo de evaluaci√≥n baseline para probar configuraciones
    
    Returns:
        str: C√≥digo Python para evaluaci√≥n
    """
    
    baseline_code = '''
# C√ìDIGO DE EVALUACI√ìN BASELINE - CLASS WEIGHTING
# Usar este c√≥digo como plantilla para evaluar diferentes configuraciones

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, f1_score, roc_auc_score
import xgboost as xgb

def evaluate_class_weighting(X_train, y_train, X_val, y_val, algorithm_configs):
    """
    Eval√∫a diferentes configuraciones de class weighting
    """
    results = {}
    
    # 1. Random Forest con diferentes configuraciones
    print("=== RANDOM FOREST ===")
    rf_configs = algorithm_configs['random_forest']['sklearn_params']
    
    for config_name, params in rf_configs.items():
        rf = RandomForestClassifier(n_estimators=100, random_state=42, **params)
        rf.fit(X_train, y_train)
        y_pred = rf.predict(X_val)
        
        f1 = f1_score(y_val, y_pred)
        auc = roc_auc_score(y_val, rf.predict_proba(X_val)[:, 1])
        
        results[f'rf_{config_name}'] = {'f1': f1, 'auc': auc}
        print(f"{config_name}: F1={f1:.3f}, AUC={auc:.3f}")
    
    # 2. XGBoost con scale_pos_weight
    print("\\n=== XGBOOST ===")
    xgb_configs = algorithm_configs['xgboost']['scale_pos_weight']
    
    for config_name, scale_weight in xgb_configs.items():
        xgb_model = xgb.XGBClassifier(
            scale_pos_weight=scale_weight,
            n_estimators=100,
            random_state=42
        )
        xgb_model.fit(X_train, y_train)
        y_pred = xgb_model.predict(X_val)
        
        f1 = f1_score(y_val, y_pred)
        auc = roc_auc_score(y_val, xgb_model.predict_proba(X_val)[:, 1])
        
        results[f'xgb_{config_name}'] = {'f1': f1, 'auc': auc}
        print(f"{config_name}: F1={f1:.3f}, AUC={auc:.3f}")
    
    # 3. Logistic Regression
    print("\\n=== LOGISTIC REGRESSION ===")
    lr_configs = algorithm_configs['logistic_regression']['sklearn_params']
    
    for config_name, params in lr_configs.items():
        lr = LogisticRegression(random_state=42, max_iter=1000, **params)
        lr.fit(X_train, y_train)
        y_pred = lr.predict(X_val)
        
        f1 = f1_score(y_val, y_pred)
        auc = roc_auc_score(y_val, lr.predict_proba(X_val)[:, 1])
        
        results[f'lr_{config_name}'] = {'f1': f1, 'auc': auc}
        print(f"{config_name}: F1={f1:.3f}, AUC={auc:.3f}")
    
    return results

# EJEMPLO DE USO:
# results = evaluate_class_weighting(X_train, y_train, X_val, y_val, algorithm_configs)
'''
    
    return baseline_code

def create_visualizations(distribution_info, weighting_strategies, timestamp):
    """
    Crea visualizaciones de las estrategias de class weighting
    
    Args:
        distribution_info (dict): Informaci√≥n de distribuci√≥n
        weighting_strategies (dict): Estrategias de weighting
        timestamp (str): Timestamp para archivos
    """
    logger = logging.getLogger(__name__)
    logger.info("GENERANDO VISUALIZACIONES DE CLASS WEIGHTING")
    logger.info("=" * 50)
    
    try:
        # 1. Gr√°fico de estrategias de weighting
        plt.figure(figsize=(12, 8))
        
        strategies = list(weighting_strategies.keys())
        class_1_weights = [weighting_strategies[s][1] for s in strategies]
        
        colors = plt.cm.Set3(np.linspace(0, 1, len(strategies)))
        bars = plt.bar(strategies, class_1_weights, color=colors, alpha=0.8, edgecolor='black')
        
        plt.ylabel('Peso de Clase Minoritaria (Churn)', fontsize=12, fontweight='bold')
        plt.xlabel('Estrategia de Weighting', fontsize=12, fontweight='bold')
        plt.title('Comparaci√≥n de Estrategias de Class Weighting\nPeso Asignado a la Clase Minoritaria (Churn)', 
                 fontsize=14, fontweight='bold', pad=20)
        
        # A√±adir valores en las barras
        for bar, weight in zip(bars, class_1_weights):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                    f'{weight:.2f}', ha='center', va='bottom', fontweight='bold')
        
        # L√≠nea de referencia para balance perfecto
        plt.axhline(y=distribution_info['imbalance_ratio'], color='red', linestyle='--', 
                   label=f'Ratio Actual: {distribution_info["imbalance_ratio"]:.2f}')
        plt.axhline(y=1.0, color='green', linestyle='--', alpha=0.5, label='Balance Perfecto: 1.0')
        
        plt.legend()
        plt.grid(axis='y', alpha=0.3)
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(f'graficos/paso4_estrategias_class_weighting_{timestamp}.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. Gr√°fico de impacto te√≥rico
        plt.figure(figsize=(12, 6))
        
        # Simular impacto en m√©tricas (te√≥rico)
        baseline_metrics = {'Precision': 0.45, 'Recall': 0.35, 'F1-Score': 0.39}
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # Subplot 1: Distribuci√≥n actual
        labels = ['No Churn', 'Churn']
        sizes = [distribution_info['class_0_percentage'], distribution_info['class_1_percentage']]
        colors_pie = ['#2E86AB', '#A23B72']
        
        ax1.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors_pie, 
               explode=(0, 0.1), shadow=True, startangle=90)
        ax1.set_title('Distribuci√≥n Actual de Clases\n(Sin Class Weighting)', fontweight='bold')
        
        # Subplot 2: M√©tricas esperadas por estrategia
        strategies_short = ['Baseline', 'Conservative', 'Balanced', 'Inverse']
        f1_expected = [0.39, 0.58, 0.62, 0.55]  # Valores esperados te√≥ricos
        
        bars2 = ax2.bar(strategies_short, f1_expected, color=['gray', '#2E86AB', '#F18F01', '#C73E1D'], alpha=0.8)
        ax2.set_ylabel('F1-Score Esperado', fontweight='bold')
        ax2.set_title('Impacto Esperado en F1-Score\npor Estrategia de Weighting', fontweight='bold')
        ax2.axhline(y=0.6, color='green', linestyle='--', alpha=0.7, label='Objetivo: 0.60')
        
        for bar, score in zip(bars2, f1_expected):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{score:.2f}', ha='center', va='bottom', fontweight='bold')
        
        ax2.legend()
        ax2.grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'graficos/paso4_impacto_class_weighting_{timestamp}.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("Visualizaciones generadas exitosamente")
        
    except Exception as e:
        logger.error(f"Error al generar visualizaciones: {str(e)}")

def generate_report(distribution_info, weighting_strategies, algorithm_configs, 
                   metrics_config, pipeline_config, baseline_code, timestamp):
    """
    Genera informe detallado de configuraci√≥n de class weighting
    
    Returns:
        str: Contenido del informe
    """
    
    report = f"""
================================================================================
TELECOMX - INFORME DE CONFIGURACI√ìN DE CLASS WEIGHTING
================================================================================
Fecha y Hora: {timestamp}
Paso: 4 - Configuraci√≥n de Class Weighting (Enfoque Conservador)

================================================================================
RESUMEN EJECUTIVO
================================================================================
‚Ä¢ Enfoque Adoptado: CONSERVADOR - Sin modificaci√≥n de datos originales
‚Ä¢ Total de Muestras: {distribution_info['total_samples']:,}
‚Ä¢ Distribuci√≥n Actual: {distribution_info['class_0_percentage']:.1f}% No Churn, {distribution_info['class_1_percentage']:.1f}% Churn
‚Ä¢ Ratio de Desbalance: {distribution_info['imbalance_ratio']:.2f}:1
‚Ä¢ Estrategia Principal: Class Weighting con configuraciones optimizadas
‚Ä¢ Justificaci√≥n: 26.5% de churn es excelente representaci√≥n, no requiere SMOTE

================================================================================
AN√ÅLISIS DE JUSTIFICACI√ìN DEL ENFOQUE CONSERVADOR
================================================================================

üéØ ¬øPOR QU√â NO USAR SMOTE O T√âCNICAS AGRESIVAS?

1. REPRESENTACI√ìN EXCELENTE DE CLASE MINORITARIA:
   ‚Ä¢ 26.5% de churn ({distribution_info['class_1_count']:,} muestras)
   ‚Ä¢ Suficientes ejemplos reales para entrenar modelos robustos
   ‚Ä¢ Riesgo m√≠nimo de underfitting en clase minoritaria

2. RATIO MANEJABLE (2.77:1):
   ‚Ä¢ Desbalance moderado, no severo
   ‚Ä¢ Algoritmos modernos manejan bien este nivel
   ‚Ä¢ Class weighting es suficiente y efectivo

3. VENTAJAS DEL ENFOQUE CONSERVADOR:
   ‚Ä¢ Mantiene autenticidad de los datos reales
   ‚Ä¢ Evita overfitting por datos sint√©ticos
   ‚Ä¢ Mejor generalizaci√≥n en producci√≥n
   ‚Ä¢ Interpretabilidad preservada

4. EVIDENCIA DE LA INDUSTRIA:
   ‚Ä¢ Telecomunicaciones: 20-30% churn es est√°ndar
   ‚Ä¢ Tu 26.5% est√° en rango √≥ptimo
   ‚Ä¢ Casos similares exitosos con class weighting

================================================================================
ESTRATEGIAS DE CLASS WEIGHTING CALCULADAS
================================================================================

Configuraciones optimizadas para ratio {distribution_info['imbalance_ratio']:.2f}:1:

"""
    
    for strategy_name, weights in weighting_strategies.items():
        ratio_str = f"{weights[1]:.2f}:1"
        effectiveness = ""
        if strategy_name == 'conservative':
            effectiveness = " ‚≠ê RECOMENDADA"
        elif strategy_name == 'balanced':
            effectiveness = " ‚úÖ EST√ÅNDAR"
        elif strategy_name == 'conservative':
            report += "√ìptima para tu caso. Balance entre correcci√≥n y estabilidad."
        elif strategy_name == 'balanced':
            report += "Est√°ndar de sklearn. Balanceo autom√°tico matem√°tico."
        elif strategy_name == 'sqrt':
            report += "Enfoque suave. Reduce agresividad del balanceo."
        elif strategy_name == 'inverse':
            report += "Proporci√≥n inversa directa. Puede ser agresivo."
        elif strategy_name == 'log':
            report += "Muy conservador. Para casos sensibles al overfitting."
    
    report += f"""

================================================================================
CONFIGURACIONES POR ALGORITMO
================================================================================

Configuraciones espec√≠ficas optimizadas para cada algoritmo:

"""
    
    for algo_name, config in algorithm_configs.items():
        report += f"""
ü§ñ {algo_name.upper().replace('_', ' ')}:
   ‚Ä¢ Configuraci√≥n Recomendada: {config['recommended']}
   ‚Ä¢ Notas T√©cnicas: {config['notes']}
   
   Par√°metros de Implementaci√≥n:"""
        
        if 'sklearn_params' in config:
            for param_name, param_value in config['sklearn_params'].items():
                report += f"""
   ‚Ä¢ {param_name}: {param_value}"""
        
        if 'scale_pos_weight' in config:
            for param_name, param_value in config['scale_pos_weight'].items():
                report += f"""
   ‚Ä¢ scale_pos_weight_{param_name}: {param_value:.2f}"""
        
        report += "\n"
    
    report += f"""
================================================================================
M√âTRICAS DE EVALUACI√ìN CONFIGURADAS
================================================================================

M√©tricas espec√≠ficas para datos con ratio {distribution_info['imbalance_ratio']:.2f}:1:

üìä M√âTRICAS PRIMARIAS (ALTA PRIORIDAD):
"""
    
    for metric in metrics_config['primary_metrics']:
        priority = metrics_config['metric_priorities'].get(metric, 'MEDIUM')
        target = metrics_config['target_thresholds'].get(metric, 'N/A')
        report += f"   ‚Ä¢ {metric.upper()}: Prioridad {priority}, Objetivo {target}\n"
    
    report += f"""
üìà M√âTRICAS SECUNDARIAS (MONITOREO):
"""
    
    for metric in metrics_config['secondary_metrics']:
        priority = metrics_config['metric_priorities'].get(metric, 'MEDIUM')
        target = metrics_config['target_thresholds'].get(metric, 'N/A')
        report += f"   ‚Ä¢ {metric.upper()}: Prioridad {priority}, Objetivo {target}\n"
    
    report += f"""
üíº M√âTRICAS DE NEGOCIO:
"""
    
    for metric in metrics_config['business_metrics']:
        report += f"   ‚Ä¢ {metric.replace('_', ' ').title()}\n"
    
    report += f"""
üéØ INTERPRETACI√ìN DE OBJETIVOS:

‚Ä¢ F1-Score ‚â• 0.60: Balance √≥ptimo entre Precision y Recall
‚Ä¢ Average Precision ‚â• 0.65: Mejor que AUC-ROC para datos desbalanceados
‚Ä¢ Recall ‚â• 0.70: Capturar al menos 70% de clientes con riesgo de churn
‚Ä¢ Precision ‚â• 0.60: Eficiencia en campa√±as de retenci√≥n
‚Ä¢ ROC-AUC ‚â• 0.75: Capacidad discriminatoria general

‚ö†Ô∏è NOTA IMPORTANTE: Accuracy NO es confiable para datos desbalanceados.
   Con tu ratio, un modelo que prediga siempre "No Churn" tendr√≠a 73.5% accuracy.

================================================================================
PIPELINE DE EVALUACI√ìN CONFIGURADO
================================================================================

Divisi√≥n estratificada de datos:

üìÇ SPLITS CONFIGURADOS:
‚Ä¢ Training: {pipeline_config['split_info']['train_size']:,} muestras ({pipeline_config['split_info']['train_churn_rate']:.1%} churn)
‚Ä¢ Validation: {pipeline_config['split_info']['val_size']:,} muestras ({pipeline_config['split_info']['val_churn_rate']:.1%} churn)
‚Ä¢ Test: {pipeline_config['split_info']['test_size']:,} muestras ({pipeline_config['split_info']['test_churn_rate']:.1%} churn)

üîÑ VALIDACI√ìN CRUZADA:
‚Ä¢ M√©todo: {pipeline_config['cv_strategy']['method']}
‚Ä¢ Folds: {pipeline_config['cv_strategy']['n_splits']}
‚Ä¢ Estratificado: S√≠ (mantiene proporci√≥n de clases)
‚Ä¢ Random State: {pipeline_config['cv_strategy']['random_state']}

‚úÖ VENTAJAS DEL SPLIT ESTRATIFICADO:
‚Ä¢ Misma proporci√≥n de churn en train/val/test
‚Ä¢ Evaluaci√≥n consistente y confiable
‚Ä¢ Previene sesgos en la evaluaci√≥n
‚Ä¢ Comparaci√≥n justa entre modelos

================================================================================
C√ìDIGO DE EVALUACI√ìN BASELINE
================================================================================

{baseline_code}

================================================================================
PLAN DE IMPLEMENTACI√ìN RECOMENDADO
================================================================================

üöÄ FASE 1: VALIDACI√ìN DE CONFIGURACIONES (INMEDIATA)
1. Implementar Random Forest con configuraci√≥n 'conservative'
2. Evaluar XGBoost con scale_pos_weight = 2.5-3.0
3. Probar Logistic Regression con class_weight='balanced'
4. Comparar m√©tricas F1-Score y Average Precision

üîç FASE 2: OPTIMIZACI√ìN FINA (SIGUIENTE SEMANA)
1. Ajustar hiperpar√°metros manteniendo class weighting
2. Probar ensemble methods con diferentes configuraciones
3. Validar con validaci√≥n cruzada completa
4. Seleccionar configuraci√≥n final

üìä FASE 3: EVALUACI√ìN FINAL (ANTES DE PRODUCCI√ìN)
1. Evaluar en conjunto de test reservado
2. An√°lisis de curvas PR y ROC
3. An√°lisis de business impact
4. Documentaci√≥n final del modelo

================================================================================
CONFIGURACIONES ESPEC√çFICAS RECOMENDADAS
================================================================================

Para tu caso espec√≠fico (26.5% churn, ratio 2.77:1):

ü•á CONFIGURACI√ìN √ìPTIMA:
```python
# Random Forest (RECOMENDADO #1)
RandomForestClassifier(
    n_estimators=200,
    class_weight={{0: 1.0, 1: 2.5}},
    random_state=42,
    n_jobs=-1
)

# XGBoost (RECOMENDADO #2)
XGBClassifier(
    scale_pos_weight=2.77,
    n_estimators=200,
    learning_rate=0.1,
    random_state=42
)

# Logistic Regression (BASELINE)
LogisticRegression(
    class_weight='balanced',
    random_state=42,
    max_iter=1000
)
```

üéØ EXPECTATIVAS DE PERFORMANCE:
‚Ä¢ F1-Score esperado: 0.58 - 0.65
‚Ä¢ Average Precision esperado: 0.62 - 0.70
‚Ä¢ Recall esperado: 0.68 - 0.75
‚Ä¢ Precision esperado: 0.55 - 0.65

================================================================================
VALIDACI√ìN Y MONITOREO
================================================================================

üìà M√âTRICAS A MONITOREAR EN PRODUCCI√ìN:
1. F1-Score mensual por segmento de clientes
2. Precision/Recall trade-off por campa√±a de retenci√≥n
3. Distribuci√≥n de scores de probabilidad
4. Drift en caracter√≠sticas de entrada

‚ö†Ô∏è SE√ëALES DE ALERTA:
‚Ä¢ F1-Score < 0.55: Revisar configuraci√≥n
‚Ä¢ Recall < 0.65: Aumentar peso de clase minoritaria
‚Ä¢ Precision < 0.50: Reducir peso de clase minoritaria
‚Ä¢ AUC-PR < 0.60: Revisar features o algoritmo

üîÑ REENTRENAMIENTO:
‚Ä¢ Frecuencia recomendada: Trimestral
‚Ä¢ Mantener misma estrategia de class weighting
‚Ä¢ Validar que distribuci√≥n de clases se mantiene estable

================================================================================
VENTAJAS COMPETITIVAS DEL ENFOQUE
================================================================================

‚úÖ VENTAJAS DE TU CONFIGURACI√ìN:
‚Ä¢ Datos reales preservados (no sint√©ticos)
‚Ä¢ Modelos interpretables y explicables
‚Ä¢ R√°pido entrenamiento (sin oversampling)
‚Ä¢ F√°cil mantenimiento en producci√≥n
‚Ä¢ Transferible a otros proyectos similares

üèÜ COMPARACI√ìN CON ALTERNATIVAS:
‚Ä¢ SMOTE: Innecesario para tu nivel de representaci√≥n
‚Ä¢ Undersampling: P√©rdida innecesaria de informaci√≥n
‚Ä¢ Cost-sensitive learning: Class weighting es m√°s simple y efectivo
‚Ä¢ Ensemble de modelos: Puede implementarse sobre esta base

================================================================================
PR√ìXIMOS PASOS RECOMENDADOS
================================================================================

üéØ PASO 5 SUGERIDO: Entrenamiento y Validaci√≥n de Modelos
‚Ä¢ Implementar configuraciones recomendadas
‚Ä¢ Ejecutar validaci√≥n cruzada estratificada  
‚Ä¢ Comparar performance entre algoritmos
‚Ä¢ Seleccionar modelo campe√≥n

üìã CHECKLIST ANTES DEL PASO 5:
‚ñ° Configuraciones de class weighting implementadas
‚ñ° Pipeline de evaluaci√≥n validado
‚ñ° M√©tricas objetivo definidas
‚ñ° Datos de test reservados y no tocados
‚ñ° C√≥digo de evaluaci√≥n baseline funcionando

================================================================================
ARCHIVOS GENERADOS
================================================================================

üìä VISUALIZACIONES:
‚Ä¢ Estrategias de weighting: graficos/paso4_estrategias_class_weighting_{timestamp}.png
‚Ä¢ Impacto esperado: graficos/paso4_impacto_class_weighting_{timestamp}.png

üìÑ DOCUMENTACI√ìN:
‚Ä¢ Informe completo: informes/paso4_configuracion_class_weighting_informe_{timestamp}.txt
‚Ä¢ Log del proceso: logs/paso4_class_weighting.log

üíª C√ìDIGO:
‚Ä¢ Pipeline de evaluaci√≥n configurado en memoria
‚Ä¢ Configuraciones por algoritmo documentadas
‚Ä¢ C√≥digo baseline para implementaci√≥n inmediata

================================================================================
CONCLUSI√ìN
================================================================================

üéØ CONCLUSI√ìN PRINCIPAL:
Tu dataset con 26.5% de churn y ratio 2.77:1 est√° en el rango √ìPTIMO para 
class weighting conservador. No necesitas t√©cnicas agresivas como SMOTE.
Las configuraciones generadas maximizar√°n el rendimiento manteniendo la 
integridad de los datos originales.

üöÄ SIGUIENTE ACCI√ìN RECOMENDADA:
Implementar Random Forest con class_weight={{0: 1.0, 1: 2.5}} como primer
modelo baseline y comparar con XGBoost usando scale_pos_weight=2.77.

================================================================================
FIN DEL INFORME
================================================================================
"""
    
    return report

def save_configurations(algorithm_configs, metrics_config, pipeline_config, timestamp):
    """
    Guarda las configuraciones en archivos JSON para uso posterior
    
    Args:
        algorithm_configs (dict): Configuraciones por algoritmo
        metrics_config (dict): Configuraci√≥n de m√©tricas
        pipeline_config (dict): Configuraci√≥n del pipeline
        timestamp (str): Timestamp para archivos
    """
    logger = logging.getLogger(__name__)
    
    import json
    
    # Preparar configuraciones para JSON (convertir numpy types)
    json_configs = {
        'algorithm_configs': {},
        'metrics_config': metrics_config,
        'pipeline_info': {
            'split_info': pipeline_config['split_info'],
            'cv_strategy': pipeline_config['cv_strategy']
        },
        'timestamp': timestamp
    }
    
    # Convertir configuraciones de algoritmos a JSON serializable
    for algo_name, config in algorithm_configs.items():
        json_configs['algorithm_configs'][algo_name] = {}
        for key, value in config.items():
            if isinstance(value, dict):
                json_configs['algorithm_configs'][algo_name][key] = {
                    k: float(v) if isinstance(v, (np.float64, np.float32)) else v 
                    for k, v in value.items()
                }
            else:
                json_configs['algorithm_configs'][algo_name][key] = value
    
    # Guardar configuraciones
    config_filename = f"informes/paso4_configuraciones_{timestamp}.json"
    with open(config_filename, 'w', encoding='utf-8') as f:
        json.dump(json_configs, f, indent=2, ensure_ascii=False)
    
    logger.info(f"Configuraciones guardadas: {config_filename}")
    return config_filename

def save_files(report_content, timestamp):
    """
    Guarda el informe en la carpeta correspondiente
    
    Args:
        report_content (str): Contenido del informe
        timestamp (str): Timestamp para nombres de archivo
    """
    logger = logging.getLogger(__name__)
    
    # Guardar informe
    report_filename = f"informes/paso4_configuracion_class_weighting_informe_{timestamp}.txt"
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write(report_content)
    logger.info(f"Informe guardado: {report_filename}")
    
    return report_filename

def main():
    """Funci√≥n principal que ejecuta todo el proceso de configuraci√≥n de class weighting"""
    
    # Crear directorios y configurar logging
    create_directories()
    logger = setup_logging()
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    logger.info("INICIANDO PASO 4: CONFIGURACION DE CLASS WEIGHTING")
    logger.info("=" * 70)
    
    try:
        # 1. Cargar datos del Paso 2
        input_file = find_latest_paso2_file()
        df = load_data(input_file)
        
        # 2. Analizar distribuci√≥n actual
        distribution_info = analyze_current_distribution(df)
        
        # 3. Calcular estrategias de class weighting
        weighting_strategies = calculate_class_weights(distribution_info)
        
        # 4. Generar configuraciones por algoritmo
        algorithm_configs = generate_algorithm_configurations(weighting_strategies, distribution_info)
        
        # 5. Configurar m√©tricas de evaluaci√≥n
        metrics_config = setup_evaluation_metrics()
        
        # 6. Crear pipeline de evaluaci√≥n
        pipeline_config = create_evaluation_pipeline(df, weighting_strategies)
        
        # 7. Generar c√≥digo baseline
        baseline_code = generate_baseline_evaluation()
        
        # 8. Crear visualizaciones
        create_visualizations(distribution_info, weighting_strategies, timestamp)
        
        # 9. Generar informe detallado
        report_content = generate_report(
            distribution_info, weighting_strategies, algorithm_configs,
            metrics_config, pipeline_config, baseline_code, timestamp
        )
        
        # 10. Guardar archivos
        report_file = save_files(report_content, timestamp)
        config_file = save_configurations(algorithm_configs, metrics_config, pipeline_config, timestamp)
        
        # 11. Resumen final
        logger.info("=" * 70)
        logger.info("PROCESO COMPLETADO EXITOSAMENTE")
        logger.info(f"Ratio de desbalance: {distribution_info['imbalance_ratio']:.2f}:1")
        logger.info(f"Enfoque: Conservador con class weighting")
        logger.info(f"Configuraciones generadas: {len(algorithm_configs)} algoritmos")
        logger.info(f"Estrategias de weighting: {len(weighting_strategies)}")
        logger.info(f"Informe generado: {report_file}")
        logger.info(f"Configuraciones JSON: {config_file}")
        logger.info("=" * 70)
        
        print(f"\nCONFIGURACION OPTIMA RECOMENDADA:")
        print(f"   ‚Ä¢ Random Forest: class_weight={{0: 1.0, 1: 2.5}}")
        print(f"   ‚Ä¢ XGBoost: scale_pos_weight={distribution_info['imbalance_ratio']:.2f}")
        print(f"   ‚Ä¢ Logistic Regression: class_weight='balanced'")
        print(f"   ‚Ä¢ Objetivo F1-Score: ‚â• 0.60")
        
        print("\nSIGUIENTE PASO:")
        print("   Paso 5: Entrenamiento y Validaci√≥n de Modelos con Class Weighting")
        
    except Exception as e:
        logger.error(f"ERROR EN EL PROCESO: {str(e)}")
        raise

if __name__ == "__main__":
    main()
        
    
   

