"""
TELECOMX - PIPELINE DE PREDICCI√ìN DE CHURN
===========================================
Paso 6: An√°lisis de Correlaci√≥n

Descripci√≥n:
    Visualiza la matriz de correlaci√≥n para identificar relaciones entre las 
    variables num√©ricas. Presta especial atenci√≥n a las variables que muestran 
    una mayor correlaci√≥n con la cancelaci√≥n, ya que estas pueden ser fuertes 
    candidatas para el modelo predictivo.

Autor: Ingeniero de Datos
Fecha: 2025-07-22
"""

import pandas as pd
import numpy as np
import os
import logging
from datetime import datetime
import sys
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.cluster.hierarchy import dendrogram, linkage
import warnings
warnings.filterwarnings('ignore')

def create_directories():
    """Crea las carpetas necesarias si no existen"""
    directories = ['script', 'informes', 'excel', 'logs', 'graficos']
    for directory in directories:
        if not os.path.exists(directory):
            os.makedirs(directory)
            print(f"Carpeta creada: {directory}")

# Configuraci√≥n de logging
def setup_logging():
    """Configura el sistema de logging para trackear el proceso"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('logs/paso6_analisis_correlacion.log', mode='a', encoding='utf-8')
        ]
    )
    return logging.getLogger(__name__)

def find_latest_paso2_file():
    """
    Encuentra el archivo m√°s reciente del Paso 2 en la carpeta excel
    
    Returns:
        str: Ruta al archivo m√°s reciente del Paso 2
    """
    logger = logging.getLogger(__name__)
    
    excel_files = [f for f in os.listdir('excel') if f.startswith('telecomx_paso2_encoding_aplicado_')]
    
    if not excel_files:
        raise FileNotFoundError("No se encontr√≥ ning√∫n archivo del Paso 2 en la carpeta excel/")
    
    # Ordenar por fecha de modificaci√≥n y tomar el m√°s reciente
    excel_files.sort(key=lambda x: os.path.getmtime(os.path.join('excel', x)), reverse=True)
    latest_file = os.path.join('excel', excel_files[0])
    
    logger.info(f"Archivo del Paso 2 encontrado: {latest_file}")
    return latest_file

def load_data(file_path):
    """
    Carga el dataset del Paso 2 con detecci√≥n autom√°tica de codificaci√≥n
    
    Args:
        file_path (str): Ruta al archivo CSV
        
    Returns:
        pd.DataFrame: Dataset cargado
    """
    logger = logging.getLogger(__name__)
    
    # Lista de codificaciones a probar
    encodings_to_try = ['utf-8-sig', 'cp1252', 'latin-1', 'iso-8859-1', 'utf-8']
    
    for encoding in encodings_to_try:
        try:
            df = pd.read_csv(file_path, sep=';', encoding=encoding)
            logger.info("Dataset cargado exitosamente desde: " + file_path)
            logger.info(f"Codificacion utilizada: {encoding}")
            logger.info(f"Dimensiones: {df.shape[0]} filas x {df.shape[1]} columnas")
            return df
        except (UnicodeDecodeError, UnicodeError):
            logger.warning(f"No se pudo cargar con codificacion {encoding}, probando siguiente...")
            continue
        except Exception as e:
            if encoding == encodings_to_try[-1]:
                logger.error("Error al cargar el dataset: " + str(e))
                raise
            else:
                continue
    
    raise ValueError(f"No se pudo cargar el archivo {file_path} con ninguna codificacion probada")

def verify_target_variable(df):
    """
    Verifica que la variable objetivo existe y es num√©rica
    
    Args:
        df (pd.DataFrame): Dataset cargado
        
    Returns:
        bool: True si la variable objetivo es v√°lida
    """
    logger = logging.getLogger(__name__)
    logger.info("VERIFICANDO VARIABLE OBJETIVO")
    logger.info("=" * 40)
    
    if 'Abandono_Cliente' not in df.columns:
        logger.error("Variable objetivo 'Abandono_Cliente' no encontrada")
        return False
    
    if not pd.api.types.is_numeric_dtype(df['Abandono_Cliente']):
        logger.error("Variable objetivo 'Abandono_Cliente' no es num√©rica")
        return False
    
    unique_vals = df['Abandono_Cliente'].unique()
    logger.info(f"Variable objetivo verificada: {len(unique_vals)} valores √∫nicos")
    logger.info(f"Valores: {sorted(unique_vals)}")
    
    return True

def calculate_correlation_matrix(df):
    """
    Calcula la matriz de correlaci√≥n completa del dataset
    
    Args:
        df (pd.DataFrame): Dataset con variables num√©ricas
        
    Returns:
        tuple: (correlation_matrix, numeric_columns)
    """
    logger = logging.getLogger(__name__)
    logger.info("CALCULANDO MATRIZ DE CORRELACION")
    logger.info("=" * 40)
    
    # Seleccionar solo columnas num√©ricas
    numeric_df = df.select_dtypes(include=[np.number])
    numeric_columns = numeric_df.columns.tolist()
    
    logger.info(f"Variables num√©ricas encontradas: {len(numeric_columns)}")
    logger.info(f"Variables: {numeric_columns}")
    
    # Calcular matriz de correlaci√≥n
    correlation_matrix = numeric_df.corr()
    
    logger.info("Matriz de correlaci√≥n calculada exitosamente")
    logger.info(f"Dimensiones: {correlation_matrix.shape}")
    
    return correlation_matrix, numeric_columns

def analyze_target_correlations(correlation_matrix, target_threshold=0.3):
    """
    Analiza las correlaciones con la variable objetivo
    
    Args:
        correlation_matrix (pd.DataFrame): Matriz de correlaci√≥n
        target_threshold (float): Umbral para correlaci√≥n significativa
        
    Returns:
        dict: An√°lisis de correlaciones con target
    """
    logger = logging.getLogger(__name__)
    logger.info("ANALIZANDO CORRELACIONES CON VARIABLE OBJETIVO")
    logger.info("=" * 50)
    
    if 'Abandono_Cliente' not in correlation_matrix.columns:
        logger.error("Variable objetivo no encontrada en matriz de correlaci√≥n")
        return {}
    
    # Correlaciones con variable objetivo
    target_correlations = correlation_matrix['Abandono_Cliente'].drop('Abandono_Cliente')
    
    # Ordenar por valor absoluto (correlaciones m√°s fuertes)
    target_correlations_abs = target_correlations.abs().sort_values(ascending=False)
    
    # Identificar correlaciones significativas
    significant_positive = target_correlations[target_correlations >= target_threshold].sort_values(ascending=False)
    significant_negative = target_correlations[target_correlations <= -target_threshold].sort_values(ascending=True)
    
    analysis = {
        'all_correlations': target_correlations,
        'correlations_by_strength': target_correlations_abs,
        'significant_positive': significant_positive,
        'significant_negative': significant_negative,
        'threshold_used': target_threshold,
        'total_significant': len(significant_positive) + len(significant_negative),
        'strongest_predictor': target_correlations_abs.index[0] if len(target_correlations_abs) > 0 else None,
        'strongest_correlation': target_correlations_abs.iloc[0] if len(target_correlations_abs) > 0 else 0
    }
    
    # Logging de resultados
    logger.info(f"Umbral de significancia: |r| >= {target_threshold}")
    logger.info(f"Variables con correlaci√≥n significativa: {analysis['total_significant']}")
    logger.info(f"Correlaciones positivas significativas: {len(significant_positive)}")
    logger.info(f"Correlaciones negativas significativas: {len(significant_negative)}")
    
    if analysis['strongest_predictor']:
        logger.info(f"Predictor m√°s fuerte: {analysis['strongest_predictor']}")
        logger.info(f"Correlaci√≥n: {analysis['strongest_correlation']:.4f}")
    
    # Log de top correlaciones
    logger.info("\nTop 10 correlaciones con Abandono_Cliente:")
    for i, (var, corr) in enumerate(target_correlations_abs.head(10).items(), 1):
        direction = "positiva" if target_correlations[var] > 0 else "negativa"
        logger.info(f"  {i:2d}. {var}: {target_correlations[var]:+.4f} ({direction})")
    
    return analysis

def detect_multicollinearity(correlation_matrix, multicollinearity_threshold=0.8):
    """
    Detecta multicolinealidad entre variables predictoras
    
    Args:
        correlation_matrix (pd.DataFrame): Matriz de correlaci√≥n
        multicollinearity_threshold (float): Umbral para detectar multicolinealidad
        
    Returns:
        dict: An√°lisis de multicolinealidad
    """
    logger = logging.getLogger(__name__)
    logger.info("DETECTANDO MULTICOLINEALIDAD")
    logger.info("=" * 35)
    
    # Excluir variable objetivo del an√°lisis
    predictor_matrix = correlation_matrix.drop('Abandono_Cliente', axis=0).drop('Abandono_Cliente', axis=1)
    
    # Encontrar pares con alta correlaci√≥n
    high_correlations = []
    
    for i in range(len(predictor_matrix.columns)):
        for j in range(i+1, len(predictor_matrix.columns)):
            var1 = predictor_matrix.columns[i]
            var2 = predictor_matrix.columns[j]
            correlation = predictor_matrix.iloc[i, j]
            
            if abs(correlation) >= multicollinearity_threshold:
                high_correlations.append({
                    'variable_1': var1,
                    'variable_2': var2,
                    'correlation': correlation,
                    'abs_correlation': abs(correlation)
                })
    
    # Ordenar por correlaci√≥n absoluta descendente
    high_correlations.sort(key=lambda x: x['abs_correlation'], reverse=True)
    
    # Identificar variables problem√°ticas
    problematic_vars = set()
    for pair in high_correlations:
        problematic_vars.add(pair['variable_1'])
        problematic_vars.add(pair['variable_2'])
    
    analysis = {
        'threshold_used': multicollinearity_threshold,
        'high_correlation_pairs': high_correlations,
        'total_problematic_pairs': len(high_correlations),
        'problematic_variables': list(problematic_vars),
        'total_problematic_vars': len(problematic_vars)
    }
    
    # Logging de resultados
    logger.info(f"Umbral de multicolinealidad: |r| >= {multicollinearity_threshold}")
    logger.info(f"Pares con alta correlaci√≥n: {len(high_correlations)}")
    logger.info(f"Variables problem√°ticas: {len(problematic_vars)}")
    
    if high_correlations:
        logger.info("\nPares con multicolinealidad detectada:")
        for i, pair in enumerate(high_correlations[:5], 1):
            logger.info(f"  {i}. {pair['variable_1']} ‚Üî {pair['variable_2']}: {pair['correlation']:+.4f}")
        
        if len(high_correlations) > 5:
            logger.info(f"  ... y {len(high_correlations) - 5} pares adicionales")
    else:
        logger.info("No se detect√≥ multicolinealidad significativa")
    
    return analysis

def generate_variable_recommendations(target_analysis, multicollinearity_analysis, correlation_matrix):
    """
    Genera recomendaciones para selecci√≥n de variables
    
    Args:
        target_analysis (dict): An√°lisis de correlaciones con target
        multicollinearity_analysis (dict): An√°lisis de multicolinealidad
        correlation_matrix (pd.DataFrame): Matriz de correlaci√≥n
        
    Returns:
        dict: Recomendaciones de variables
    """
    logger = logging.getLogger(__name__)
    logger.info("GENERANDO RECOMENDACIONES DE VARIABLES")
    logger.info("=" * 45)
    
    all_variables = correlation_matrix.columns.tolist()
    if 'Abandono_Cliente' in all_variables:
        all_variables.remove('Abandono_Cliente')
    
    recommendations = {
        'high_priority': [],
        'medium_priority': [],
        'low_priority': [],
        'consider_removal': [],
        'reasoning': {}
    }
    
    # Clasificar variables seg√∫n correlaci√≥n con target
    for var in all_variables:
        target_corr = abs(correlation_matrix.loc[var, 'Abandono_Cliente'])
        
        # Verificar si est√° en pares multicolineales
        is_multicollinear = var in multicollinearity_analysis['problematic_variables']
        
        # Asignar prioridad
        if target_corr >= 0.3:
            if is_multicollinear:
                recommendations['consider_removal'].append(var)
                recommendations['reasoning'][var] = f"Alta correlaci√≥n con target ({target_corr:.3f}) pero multicolineal - evaluar cu√°l mantener"
            else:
                recommendations['high_priority'].append(var)
                recommendations['reasoning'][var] = f"Alta correlaci√≥n con target ({target_corr:.3f}) y sin multicolinealidad"
        
        elif target_corr >= 0.1:
            if is_multicollinear:
                recommendations['consider_removal'].append(var)
                recommendations['reasoning'][var] = f"Correlaci√≥n moderada ({target_corr:.3f}) pero multicolineal - considerar eliminaci√≥n"
            else:
                recommendations['medium_priority'].append(var)
                recommendations['reasoning'][var] = f"Correlaci√≥n moderada con target ({target_corr:.3f})"
        
        else:
            if is_multicollinear:
                recommendations['consider_removal'].append(var)
                recommendations['reasoning'][var] = f"Baja correlaci√≥n ({target_corr:.3f}) y multicolineal - candidato para eliminaci√≥n"
            else:
                recommendations['low_priority'].append(var)
                recommendations['reasoning'][var] = f"Baja correlaci√≥n con target ({target_corr:.3f})"
    
    # Logging de recomendaciones
    logger.info(f"Variables de alta prioridad: {len(recommendations['high_priority'])}")
    logger.info(f"Variables de prioridad media: {len(recommendations['medium_priority'])}")
    logger.info(f"Variables de baja prioridad: {len(recommendations['low_priority'])}")
    logger.info(f"Variables a considerar eliminaci√≥n: {len(recommendations['consider_removal'])}")
    
    logger.info("\nVariables de ALTA PRIORIDAD:")
    for var in recommendations['high_priority']:
        logger.info(f"  ‚Ä¢ {var}")
    
    if recommendations['consider_removal']:
        logger.info("\nVariables a CONSIDERAR ELIMINACI√ìN:")
        for var in recommendations['consider_removal']:
            logger.info(f"  ‚Ä¢ {var}")
    
    return recommendations

def create_correlation_visualizations(correlation_matrix, target_analysis, 
                                    multicollinearity_analysis, timestamp):
    """
    Crea visualizaciones de an√°lisis de correlaci√≥n
    
    Args:
        correlation_matrix (pd.DataFrame): Matriz de correlaci√≥n
        target_analysis (dict): An√°lisis de correlaciones con target
        multicollinearity_analysis (dict): An√°lisis de multicolinealidad
        timestamp (str): Timestamp para archivos
    """
    logger = logging.getLogger(__name__)
    logger.info("GENERANDO VISUALIZACIONES DE CORRELACION")
    logger.info("=" * 50)
    
    try:
        # 1. Heatmap completo de correlaci√≥n
        plt.figure(figsize=(14, 12))
        
        # Crear m√°scara para tri√°ngulo superior
        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
        
        # Crear heatmap
        sns.heatmap(correlation_matrix, 
                   mask=mask,
                   annot=True, 
                   cmap='RdBu_r', 
                   center=0,
                   square=True,
                   fmt='.2f',
                   cbar_kws={"shrink": .8},
                   annot_kws={'size': 8})
        
        plt.title('Matriz de Correlaci√≥n Completa\nTri√°ngulo Inferior', 
                 fontsize=16, fontweight='bold', pad=20)
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        plt.savefig(f'graficos/paso6_matriz_correlacion_completa_{timestamp}.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. Correlaciones con variable objetivo (ranking)
        plt.figure(figsize=(12, 8))
        
        target_corr = target_analysis['all_correlations'].sort_values(key=abs, ascending=True)
        
        # Colores seg√∫n signo de correlaci√≥n
        colors = ['red' if x < 0 else 'blue' for x in target_corr.values]
        
        bars = plt.barh(range(len(target_corr)), target_corr.values, color=colors, alpha=0.7)
        
        plt.yticks(range(len(target_corr)), target_corr.index)
        plt.xlabel('Correlaci√≥n con Abandono_Cliente', fontsize=12, fontweight='bold')
        plt.title('Ranking de Correlaciones con Variable Objetivo\n(Ordenado por magnitud)', 
                 fontsize=14, fontweight='bold', pad=20)
        
        # A√±adir l√≠neas de referencia
        plt.axvline(x=0.3, color='green', linestyle='--', alpha=0.7, label='Umbral significativo (+0.3)')
        plt.axvline(x=-0.3, color='green', linestyle='--', alpha=0.7, label='Umbral significativo (-0.3)')
        plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)
        
        # A√±adir valores en las barras
        for i, bar in enumerate(bars):
            width = bar.get_width()
            plt.text(width + 0.01 if width >= 0 else width - 0.01, 
                    bar.get_y() + bar.get_height()/2, 
                    f'{width:.3f}', 
                    ha='left' if width >= 0 else 'right', 
                    va='center', fontsize=8)
        
        plt.legend()
        plt.grid(axis='x', alpha=0.3)
        plt.tight_layout()
        plt.savefig(f'graficos/paso6_ranking_correlaciones_target_{timestamp}.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. Heatmap espec√≠fico de variables m√°s correlacionadas con target
        top_variables = target_analysis['correlations_by_strength'].head(15).index.tolist()
        top_variables.append('Abandono_Cliente')
        
        if len(top_variables) > 1:
            plt.figure(figsize=(12, 10))
            
            top_corr_matrix = correlation_matrix.loc[top_variables, top_variables]
            
            sns.heatmap(top_corr_matrix, 
                       annot=True, 
                       cmap='RdBu_r', 
                       center=0,
                       square=True,
                       fmt='.3f',
                       cbar_kws={"shrink": .8})
            
            plt.title('Correlaciones: Top 15 Variables vs Abandono_Cliente\n(Variables m√°s correlacionadas con target)', 
                     fontsize=14, fontweight='bold', pad=20)
            plt.xticks(rotation=45, ha='right')
            plt.yticks(rotation=0)
            plt.tight_layout()
            plt.savefig(f'graficos/paso6_top_correlaciones_target_{timestamp}.png', 
                       dpi=300, bbox_inches='tight')
            plt.close()
        
        # 4. An√°lisis de multicolinealidad (si existe)
        if multicollinearity_analysis['high_correlation_pairs']:
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
            
            # Subplot 1: Distribuci√≥n de correlaciones
            all_corr_values = []
            for i in range(len(correlation_matrix.columns)):
                for j in range(i+1, len(correlation_matrix.columns)):
                    if correlation_matrix.columns[i] != 'Abandono_Cliente' and correlation_matrix.columns[j] != 'Abandono_Cliente':
                        all_corr_values.append(abs(correlation_matrix.iloc[i, j]))
            
            ax1.hist(all_corr_values, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
            ax1.axvline(x=0.8, color='red', linestyle='--', label='Umbral multicolinealidad (0.8)')
            ax1.set_xlabel('Correlaci√≥n Absoluta')
            ax1.set_ylabel('Frecuencia')
            ax1.set_title('Distribuci√≥n de Correlaciones\nentre Variables Predictoras')
            ax1.legend()
            ax1.grid(alpha=0.3)
            
            # Subplot 2: Top pares multicolineales
            if len(multicollinearity_analysis['high_correlation_pairs']) > 0:
                top_pairs = multicollinearity_analysis['high_correlation_pairs'][:10]
                pair_labels = [f"{p['variable_1'][:15]}...\n{p['variable_2'][:15]}..." 
                              if len(p['variable_1']) > 15 or len(p['variable_2']) > 15 
                              else f"{p['variable_1']}\n{p['variable_2']}" 
                              for p in top_pairs]
                correlations = [p['correlation'] for p in top_pairs]
                
                colors_mult = ['red' if abs(c) >= 0.9 else 'orange' for c in correlations]
                
                bars = ax2.barh(range(len(correlations)), correlations, color=colors_mult, alpha=0.7)
                ax2.set_yticks(range(len(correlations)))
                ax2.set_yticklabels(pair_labels, fontsize=8)
                ax2.set_xlabel('Correlaci√≥n')
                ax2.set_title('Top Pares Multicolineales\n(Variables Predictoras)')
                ax2.grid(axis='x', alpha=0.3)
                
                # A√±adir valores
                for i, bar in enumerate(bars):
                    width = bar.get_width()
                    ax2.text(width + 0.01 if width >= 0 else width - 0.01, 
                            bar.get_y() + bar.get_height()/2, 
                            f'{width:.3f}', 
                            ha='left' if width >= 0 else 'right', 
                            va='center', fontsize=8)
            
            plt.suptitle('An√°lisis de Multicolinealidad entre Variables Predictoras', 
                        fontsize=16, fontweight='bold')
            plt.tight_layout()
            plt.savefig(f'graficos/paso6_analisis_multicolinealidad_{timestamp}.png', 
                       dpi=300, bbox_inches='tight')
            plt.close()
        
        logger.info("Todas las visualizaciones generadas exitosamente")
        
    except Exception as e:
        logger.error(f"Error al generar visualizaciones: {str(e)}")

def generate_optimized_variable_list(recommendations, correlation_matrix):
    """
    Genera lista optimizada de variables para modelado
    
    Args:
        recommendations (dict): Recomendaciones de variables
        correlation_matrix (pd.DataFrame): Matriz de correlaci√≥n
        
    Returns:
        dict: Lista optimizada y configuraciones
    """
    logger = logging.getLogger(__name__)
    logger.info("GENERANDO LISTA OPTIMIZADA DE VARIABLES")
    logger.info("=" * 50)
    
    # Lista optimizada: alta y media prioridad
    optimized_features = recommendations['high_priority'] + recommendations['medium_priority']
    
    # Configuraciones para diferentes escenarios
    configurations = {
        'full_features': {
            'variables': correlation_matrix.columns.tolist(),
            'count': len(correlation_matrix.columns),
            'description': 'Todas las variables disponibles'
        },
        
        'optimized_features': {
            'variables': optimized_features + ['Abandono_Cliente'],
            'count': len(optimized_features) + 1,
            'description': 'Variables con correlaci√≥n significativa (alta y media prioridad)'
        },
        
        'high_priority_only': {
            'variables': recommendations['high_priority'] + ['Abandono_Cliente'],
            'count': len(recommendations['high_priority']) + 1,
            'description': 'Solo variables de alta prioridad (correlaci√≥n >= 0.3)'
        },
        
        'top_10_features': {
            'variables': correlation_matrix['Abandono_Cliente'].abs().nlargest(11).index.tolist(),  # Top 10 + target
            'count': 11,
            'description': 'Top 10 variables m√°s correlacionadas con target'
        }
    }
    
    # Remover variable objetivo de listas donde no corresponde
    for config_name, config in configurations.items():
        if config_name != 'full_features':
            variables_only = [v for v in config['variables'] if v != 'Abandono_Cliente']
            config['variables_only'] = variables_only
            config['feature_count'] = len(variables_only)
    
    optimization_summary = {
        'configurations': configurations,
        'recommended_config': 'optimized_features',
        'reduction_achieved': {
            'original_features': len(correlation_matrix.columns) - 1,  # Excluir target
            'optimized_features': len(optimized_features),
            'reduction_percentage': (1 - len(optimized_features) / (len(correlation_matrix.columns) - 1)) * 100
        }
    }
    
    logger.info("Configuraciones generadas:")
    for config_name, config in configurations.items():
        feature_count = config.get('feature_count', config['count'] - 1)
        logger.info(f"  {config_name}: {feature_count} variables - {config['description']}")
    
    logger.info(f"\nConfiguraci√≥n recomendada: {optimization_summary['recommended_config']}")
    logger.info(f"Reducci√≥n de dimensionalidad: {optimization_summary['reduction_achieved']['reduction_percentage']:.1f}%")
    
    return optimization_summary

def generate_report(correlation_matrix, target_analysis, multicollinearity_analysis, 
                   recommendations, optimization_summary, timestamp):
    """
    Genera informe detallado del an√°lisis de correlaci√≥n
    
    Returns:
        str: Contenido del informe
    """
    
    report = f"""
================================================================================
TELECOMX - INFORME DE AN√ÅLISIS DE CORRELACI√ìN
================================================================================
Fecha y Hora: {timestamp}
Paso: 6 - An√°lisis de Correlaci√≥n

================================================================================
RESUMEN EJECUTIVO
================================================================================
‚Ä¢ Total de Variables Analizadas: {len(correlation_matrix.columns)}
‚Ä¢ Variables Predictoras: {len(correlation_matrix.columns) - 1}
‚Ä¢ Variables con Correlaci√≥n Significativa (|r| >= 0.3): {target_analysis['total_significant']}
‚Ä¢ Predictor M√°s Fuerte: {target_analysis['strongest_predictor']}
‚Ä¢ Correlaci√≥n M√°s Fuerte: {target_analysis['strongest_correlation']:.4f}
‚Ä¢ Pares con Multicolinealidad (|r| >= 0.8): {multicollinearity_analysis['total_problematic_pairs']}
‚Ä¢ Variables Recomendadas para Modelado: {len(recommendations['high_priority']) + len(recommendations['medium_priority'])}
‚Ä¢ Reducci√≥n de Dimensionalidad Lograda: {optimization_summary['reduction_achieved']['reduction_percentage']:.1f}%

================================================================================
AN√ÅLISIS DE CORRELACIONES CON VARIABLE OBJETIVO
================================================================================

üéØ CORRELACIONES SIGNIFICATIVAS (|r| >= {target_analysis['threshold_used']}):

üìà CORRELACIONES POSITIVAS FUERTES:
"""
    
    if len(target_analysis['significant_positive']) > 0:
        for var, corr in target_analysis['significant_positive'].items():
            report += f"   ‚Ä¢ {var}: +{corr:.4f}\n"
    else:
        report += "   ‚Ä¢ No se encontraron correlaciones positivas significativas\n"
    
    report += f"""
üìâ CORRELACIONES NEGATIVAS FUERTES:
"""
    
    if len(target_analysis['significant_negative']) > 0:
        for var, corr in target_analysis['significant_negative'].items():
            report += f"   ‚Ä¢ {var}: {corr:.4f}\n"
    else:
        report += "   ‚Ä¢ No se encontraron correlaciones negativas significativas\n"
    
    report += f"""
üèÜ TOP 10 PREDICTORES M√ÅS FUERTES (por correlaci√≥n absoluta):
"""
    
    for i, (var, corr_abs) in enumerate(target_analysis['correlations_by_strength'].head(10).items(), 1):
        actual_corr = target_analysis['all_correlations'][var]
        direction = "‚ÜóÔ∏è" if actual_corr > 0 else "‚ÜòÔ∏è"
        report += f"   {i:2d}. {var}: {actual_corr:+.4f} {direction}\n"
    
    report += f"""
================================================================================
AN√ÅLISIS DE MULTICOLINEALIDAD
================================================================================

‚ö†Ô∏è DETECCI√ìN DE VARIABLES ALTAMENTE CORRELACIONADAS (|r| >= {multicollinearity_analysis['threshold_used']}):

Total de pares problem√°ticos: {multicollinearity_analysis['total_problematic_pairs']}
Variables involucradas: {multicollinearity_analysis['total_problematic_vars']}

"""
    
    if multicollinearity_analysis['high_correlation_pairs']:
        report += "üî¥ PARES CON MULTICOLINEALIDAD DETECTADA:\n"
        for i, pair in enumerate(multicollinearity_analysis['high_correlation_pairs'][:10], 1):
            severity = "üî¥ CR√çTICA" if pair['abs_correlation'] >= 0.95 else "üü° ALTA"
            report += f"   {i:2d}. {pair['variable_1']} ‚Üî {pair['variable_2']}: {pair['correlation']:+.4f} ({severity})\n"
        
        if len(multicollinearity_analysis['high_correlation_pairs']) > 10:
            remaining = len(multicollinearity_analysis['high_correlation_pairs']) - 10
            report += f"   ... y {remaining} pares adicionales\n"
    else:
        report += "‚úÖ No se detect√≥ multicolinealidad significativa entre variables predictoras.\n"
    
    report += f"""
================================================================================
RECOMENDACIONES DE SELECCI√ìN DE VARIABLES
================================================================================

üöÄ VARIABLES DE ALTA PRIORIDAD ({len(recommendations['high_priority'])} variables):
   Correlaci√≥n significativa con target (|r| >= 0.3) y sin multicolinealidad
"""
    
    for var in recommendations['high_priority']:
        target_corr = target_analysis['all_correlations'][var]
        report += f"   ‚úÖ {var}: {target_corr:+.4f}\n"
    
    report += f"""
üìä VARIABLES DE PRIORIDAD MEDIA ({len(recommendations['medium_priority'])} variables):
   Correlaci√≥n moderada con target (0.1 <= |r| < 0.3) y sin multicolinealidad
"""
    
    for var in recommendations['medium_priority'][:10]:  # Mostrar m√°ximo 10
        target_corr = target_analysis['all_correlations'][var]
        report += f"   üî∂ {var}: {target_corr:+.4f}\n"
    
    if len(recommendations['medium_priority']) > 10:
        report += f"   ... y {len(recommendations['medium_priority']) - 10} variables adicionales\n"
    
    report += f"""
‚ö†Ô∏è VARIABLES A CONSIDERAR ELIMINACI√ìN ({len(recommendations['consider_removal'])} variables):
   Variables con multicolinealidad o baja correlaci√≥n con target
"""
    
    for var in recommendations['consider_removal'][:10]:  # Mostrar m√°ximo 10
        target_corr = target_analysis['all_correlations'][var]
        reason = recommendations['reasoning'][var]
        report += f"   üî∂ {var}: {target_corr:+.4f} - {reason}\n"
    
    if len(recommendations['consider_removal']) > 10:
        report += f"   ... y {len(recommendations['consider_removal']) - 10} variables adicionales\n"
    
    report += f"""
üîª VARIABLES DE BAJA PRIORIDAD ({len(recommendations['low_priority'])} variables):
   Correlaci√≥n baja con target (|r| < 0.1) pero sin problemas de multicolinealidad
"""
    
    for var in recommendations['low_priority'][:5]:  # Mostrar m√°ximo 5
        target_corr = target_analysis['all_correlations'][var]
        report += f"   üîª {var}: {target_corr:+.4f}\n"
    
    if len(recommendations['low_priority']) > 5:
        report += f"   ... y {len(recommendations['low_priority']) - 5} variables adicionales\n"
    
    report += f"""
================================================================================
CONFIGURACIONES OPTIMIZADAS PARA MODELADO
================================================================================

üéØ CONFIGURACIONES DISPONIBLES:
"""
    
    for config_name, config in optimization_summary['configurations'].items():
        feature_count = config.get('feature_count', config['count'] - 1)
        recommended_mark = " ‚≠ê RECOMENDADA" if config_name == optimization_summary['recommended_config'] else ""
        report += f"""
üìã {config_name.upper().replace('_', ' ')}{recommended_mark}:
   ‚Ä¢ N√∫mero de variables: {feature_count}
   ‚Ä¢ Descripci√≥n: {config['description']}
   ‚Ä¢ Variables incluidas: {', '.join(config.get('variables_only', config['variables'])[:5])}{'...' if len(config.get('variables_only', config['variables'])) > 5 else ''}
"""
    
    report += f"""
================================================================================
AN√ÅLISIS DE REDUCCI√ìN DE DIMENSIONALIDAD
================================================================================

üìä IMPACTO DE LA OPTIMIZACI√ìN:

‚Ä¢ Variables originales: {optimization_summary['reduction_achieved']['original_features']}
‚Ä¢ Variables optimizadas: {optimization_summary['reduction_achieved']['optimized_features']}
‚Ä¢ Reducci√≥n lograda: {optimization_summary['reduction_achieved']['reduction_percentage']:.1f}%

‚úÖ BENEFICIOS DE LA REDUCCI√ìN:
‚Ä¢ Menor riesgo de overfitting
‚Ä¢ Modelos m√°s interpretables
‚Ä¢ Entrenamiento m√°s r√°pido
‚Ä¢ Reducci√≥n de ruido en predicciones
‚Ä¢ Menos problemas de multicolinealidad

üìà CALIDAD DE LA SELECCI√ìN:
‚Ä¢ Variables seleccionadas tienen correlaci√≥n >= 0.1 con target
‚Ä¢ Se eliminaron variables con multicolinealidad problem√°tica
‚Ä¢ Se preservaron los predictores m√°s fuertes
‚Ä¢ Balance entre performance y simplicidad

================================================================================
INTERPRETACI√ìN DE CORRELACIONES PRINCIPALES
================================================================================

üîç AN√ÅLISIS DE LOS PREDICTORES M√ÅS FUERTES:
"""
    
    # Analizar top 5 predictores
    top_5_predictors = target_analysis['correlations_by_strength'].head(5)
    
    for i, (var, corr_abs) in enumerate(top_5_predictors.items(), 1):
        actual_corr = target_analysis['all_correlations'][var]
        
        # Interpretaci√≥n del tipo de variable
        if 'encoded' in var.lower() or any(x in var for x in ['_0', '_1', '_Si', '_No']):
            var_type = "Variable categ√≥rica encoded"
        elif any(x in var.lower() for x in ['cargo', 'facturacion', 'meses']):
            var_type = "Variable num√©rica de negocio"
        else:
            var_type = "Variable predictora"
        
        # Interpretaci√≥n de la correlaci√≥n
        if actual_corr > 0:
            interpretation = "Mayor valor ‚Üí Mayor probabilidad de churn"
        else:
            interpretation = "Mayor valor ‚Üí Menor probabilidad de churn"
        
        # Magnitud de la correlaci√≥n
        if corr_abs >= 0.5:
            magnitude = "MUY FUERTE"
        elif corr_abs >= 0.3:
            magnitude = "FUERTE"
        elif corr_abs >= 0.1:
            magnitude = "MODERADA"
        else:
            magnitude = "D√âBIL"
        
        report += f"""
{i}. {var} (Correlaci√≥n: {actual_corr:+.4f} - {magnitude}):
   ‚Ä¢ Tipo: {var_type}
   ‚Ä¢ Interpretaci√≥n: {interpretation}
   ‚Ä¢ Relevancia: {'Predictor clave' if corr_abs >= 0.3 else 'Predictor secundario'}
"""
    
    report += f"""
================================================================================
RECOMENDACIONES PARA PR√ìXIMOS PASOS
================================================================================

üöÄ PASO 7 SUGERIDO: Entrenamiento de Modelos con Variables Optimizadas

üìã CONFIGURACI√ìN RECOMENDADA:
‚Ä¢ Usar configuraci√≥n: {optimization_summary['recommended_config']}
‚Ä¢ Variables a incluir: {optimization_summary['configurations'][optimization_summary['recommended_config']]['feature_count']}
‚Ä¢ Algoritmos: Random Forest + XGBoost (tree-based, manejan correlaciones bien)
‚Ä¢ Class weighting: Aplicar configuraciones del Paso 4

üîß PIPELINE DE MODELADO:
1. Cargar datos del Paso 2
2. Seleccionar variables optimizadas
3. Aplicar split estratificado (Paso 4)
4. Entrenar modelos con class weighting
5. Evaluar con m√©tricas especializadas (F1-Score, AUC-PR)

‚öñÔ∏è VALIDACI√ìN DE SELECCI√ìN:
‚Ä¢ Comparar performance: Todas las variables vs Variables optimizadas
‚Ä¢ Verificar que reducci√≥n no afecta m√©tricas principales
‚Ä¢ Confirmar mejora en tiempo de entrenamiento
‚Ä¢ Validar interpretabilidad de modelo resultante

================================================================================
CONSIDERACIONES T√âCNICAS
================================================================================

üéØ UMBRALES UTILIZADOS:
‚Ä¢ Correlaci√≥n significativa con target: |r| >= {target_analysis['threshold_used']}
‚Ä¢ Multicolinealidad entre predictores: |r| >= {multicollinearity_analysis['threshold_used']}

‚úÖ VALIDACIONES REALIZADAS:
‚Ä¢ Matriz de correlaci√≥n calculada correctamente
‚Ä¢ Variable objetivo verificada como num√©rica
‚Ä¢ An√°lisis de significancia estad√≠stica aplicado
‚Ä¢ Detecci√≥n sistem√°tica de multicolinealidad

üìä CALIDAD DE LOS DATOS:
‚Ä¢ Variables num√©ricas: {len(correlation_matrix.columns)}
‚Ä¢ Sin valores faltantes en correlaciones
‚Ä¢ Distribuci√≥n de correlaciones analizada
‚Ä¢ Patrones de relaci√≥n identificados

================================================================================
ARCHIVOS GENERADOS
================================================================================

üìä VISUALIZACIONES:
‚Ä¢ Matriz completa: graficos/paso6_matriz_correlacion_completa_{timestamp}.png
‚Ä¢ Ranking con target: graficos/paso6_ranking_correlaciones_target_{timestamp}.png
‚Ä¢ Top correlaciones: graficos/paso6_top_correlaciones_target_{timestamp}.png
‚Ä¢ An√°lisis multicolinealidad: graficos/paso6_analisis_multicolinealidad_{timestamp}.png

üìÑ DOCUMENTACI√ìN:
‚Ä¢ Informe completo: informes/paso6_analisis_correlacion_informe_{timestamp}.txt
‚Ä¢ Log del proceso: logs/paso6_analisis_correlacion.log

üíæ CONFIGURACIONES:
‚Ä¢ Lista optimizada de variables generada
‚Ä¢ Configuraciones m√∫ltiples disponibles
‚Ä¢ Recomendaciones espec√≠ficas documentadas

================================================================================
CONCLUSIONES Y SIGUIENTE PASO
================================================================================

üéØ CONCLUSIONES PRINCIPALES:

1. CALIDAD DE PREDICTORES:
   ‚Ä¢ {target_analysis['total_significant']} variables con correlaci√≥n significativa
   ‚Ä¢ Predictor m√°s fuerte: {target_analysis['strongest_predictor']} (r = {target_analysis['strongest_correlation']:.4f})
   ‚Ä¢ Distribuci√≥n balanceada de correlaciones positivas y negativas

2. OPTIMIZACI√ìN LOGRADA:
   ‚Ä¢ Reducci√≥n de {optimization_summary['reduction_achieved']['reduction_percentage']:.1f}% en dimensionalidad
   ‚Ä¢ Variables seleccionadas mantienen capacidad predictiva
   ‚Ä¢ Eliminaci√≥n de redundancia por multicolinealidad

3. PREPARACI√ìN PARA MODELADO:
   ‚Ä¢ Dataset optimizado listo para algoritmos tree-based
   ‚Ä¢ Variables interpretables para stakeholders
   ‚Ä¢ Balance entre performance y simplicidad

üìã PR√ìXIMO PASO RECOMENDADO:
Paso 7: Entrenamiento y Validaci√≥n de Modelos
‚Ä¢ Implementar Random Forest y XGBoost
‚Ä¢ Usar variables de configuraci√≥n optimizada
‚Ä¢ Aplicar class weighting del Paso 4
‚Ä¢ Evaluar con m√©tricas del Paso 3

================================================================================
FIN DEL INFORME
================================================================================
"""
    
    return report

def save_files(report_content, optimization_summary, timestamp):
    """
    Guarda el informe y las configuraciones optimizadas
    
    Args:
        report_content (str): Contenido del informe
        optimization_summary (dict): Resumen de optimizaci√≥n
        timestamp (str): Timestamp para nombres de archivo
    """
    logger = logging.getLogger(__name__)
    
    # Guardar informe
    report_filename = f"informes/paso6_analisis_correlacion_informe_{timestamp}.txt"
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write(report_content)
    logger.info(f"Informe guardado: {report_filename}")
    
    # Guardar configuraciones optimizadas en JSON
    import json
    config_filename = f"informes/paso6_configuraciones_optimizadas_{timestamp}.json"
    with open(config_filename, 'w', encoding='utf-8') as f:
        json.dump(optimization_summary, f, indent=2, ensure_ascii=False)
    logger.info(f"Configuraciones guardadas: {config_filename}")
    
    return report_filename, config_filename

def main():
    """Funci√≥n principal que ejecuta todo el proceso de an√°lisis de correlaci√≥n"""
    
    # Crear directorios y configurar logging
    create_directories()
    logger = setup_logging()
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    logger.info("INICIANDO PASO 6: ANALISIS DE CORRELACION")
    logger.info("=" * 70)
    
    try:
        # 1. Cargar datos del Paso 2
        input_file = find_latest_paso2_file()
        df = load_data(input_file)
        
        # 2. Verificar variable objetivo
        if not verify_target_variable(df):
            raise ValueError("Variable objetivo no v√°lida")
        
        # 3. Calcular matriz de correlaci√≥n
        correlation_matrix, numeric_columns = calculate_correlation_matrix(df)
        
        # 4. Analizar correlaciones con target
        target_analysis = analyze_target_correlations(correlation_matrix, target_threshold=0.3)
        
        # 5. Detectar multicolinealidad
        multicollinearity_analysis = detect_multicollinearity(correlation_matrix, multicollinearity_threshold=0.8)
        
        # 6. Generar recomendaciones de variables
        recommendations = generate_variable_recommendations(target_analysis, multicollinearity_analysis, correlation_matrix)
        
        # 7. Crear lista optimizada
        optimization_summary = generate_optimized_variable_list(recommendations, correlation_matrix)
        
        # 8. Crear visualizaciones
        create_correlation_visualizations(correlation_matrix, target_analysis, 
                                        multicollinearity_analysis, timestamp)
        
        # 9. Generar informe detallado
        report_content = generate_report(correlation_matrix, target_analysis, 
                                       multicollinearity_analysis, recommendations,
                                       optimization_summary, timestamp)
        
        # 10. Guardar archivos
        report_file, config_file = save_files(report_content, optimization_summary, timestamp)
        
        # 11. Resumen final
        logger.info("=" * 70)
        logger.info("PROCESO COMPLETADO EXITOSAMENTE")
        logger.info(f"Variables analizadas: {len(correlation_matrix.columns)}")
        logger.info(f"Correlaciones significativas: {target_analysis['total_significant']}")
        logger.info(f"Predictor m√°s fuerte: {target_analysis['strongest_predictor']}")
        logger.info(f"Correlaci√≥n m√°s fuerte: {target_analysis['strongest_correlation']:.4f}")
        logger.info(f"Pares multicolineales: {multicollinearity_analysis['total_problematic_pairs']}")
        logger.info(f"Variables optimizadas: {optimization_summary['reduction_achieved']['optimized_features']}")
        logger.info(f"Reducci√≥n: {optimization_summary['reduction_achieved']['reduction_percentage']:.1f}%")
        logger.info(f"Informe generado: {report_file}")
        logger.info(f"Configuraciones: {config_file}")
        logger.info("=" * 70)
        
        print(f"\nRESULTADOS DE CORRELACION:")
        print(f"   ‚Ä¢ Predictor m√°s fuerte: {target_analysis['strongest_predictor']}")
        print(f"   ‚Ä¢ Correlaci√≥n: {target_analysis['strongest_correlation']:.4f}")
        print(f"   ‚Ä¢ Variables significativas: {target_analysis['total_significant']}")
        print(f"   ‚Ä¢ Variables optimizadas: {optimization_summary['reduction_achieved']['optimized_features']}")
        print(f"   ‚Ä¢ Reducci√≥n de dimensionalidad: {optimization_summary['reduction_achieved']['reduction_percentage']:.1f}%")
        
        print("\nSIGUIENTE PASO:")
        print("   Paso 7: Entrenamiento y Validaci√≥n de Modelos")
        print("   (Usar configuraci√≥n optimizada de variables)")
        
    except Exception as e:
        logger.error(f"ERROR EN EL PROCESO: {str(e)}")
        raise

if __name__ == "__main__":
    main()